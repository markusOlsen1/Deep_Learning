{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAva8TnYFtFu"
      },
      "source": [
        "# Contents and why we need this lab\n",
        "\n",
        "This lab is about implementing neural networks yourself before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates. \n",
        "\n",
        "All the frameworks for deep learning you will meet from now on uses automatic differentiation (autodiff) so you don't have to code the backward step yourself. In this version of this lab you will develop your own autodif implementation. We also have a [version](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_NumPy/2.1-FNN-NumPy.ipynb) of this lab where you have to code the backward pass explicitly in Numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCa7HzwpFtFy"
      },
      "source": [
        "# External sources of information\n",
        "\n",
        "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
        "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search.\n",
        "3. [Nanograd](https://github.com/rasmusbergpalm/nanograd) is a minimalistic version of autodiff developed by Rasmus Berg Palm that we use for our framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SjiIp-TFtF0"
      },
      "source": [
        "# This notebook will follow the next steps:\n",
        "\n",
        "1. Nanograd automatic differentiation framework\n",
        "2. Finite difference method\n",
        "3. Data generation\n",
        "4. Defining and initializing the network\n",
        "5. Forward pass\n",
        "6. Training loop \n",
        "7. Testing your model\n",
        "8. Further extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyXeAA-HuT7s"
      },
      "source": [
        "# Nanograd automatic differention framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6UWKCLKubgA"
      },
      "source": [
        "The [Nanograd](https://github.com/rasmusbergpalm/nanograd) framework defines a class Var which both holds a value and gradient value that we can use to store the intermediate values when we apply the chain rule of differentiation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "Jd4CoEBNzNWS"
      },
      "outputs": [],
      "source": [
        "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\n",
        "\n",
        "from math import exp, log, tanh\n",
        "import numpy as np\n",
        "\n",
        "class Var:\n",
        "    \"\"\"\n",
        "    A variable which holds a float and enables gradient computations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, val: float, grad_fn=lambda: []):\n",
        "        assert type(val) == float\n",
        "        self.v = val\n",
        "        self.grad_fn = grad_fn\n",
        "        self.grad = 0.0\n",
        "\n",
        "    def backprop(self, bp):\n",
        "        self.grad += bp\n",
        "\n",
        "        for input, grad in self.grad_fn():\n",
        "            input.backprop(grad * bp)\n",
        "\n",
        "    def backward(self):\n",
        "        self.backprop(1.0)\n",
        "\n",
        "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
        "\n",
        "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
        "\n",
        "    def __pow__(self, power):\n",
        "        assert type(power) in {float, int}, \"power must be float or int\"\n",
        "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
        "\n",
        "    def __neg__(self: 'Var') -> 'Var':\n",
        "        return Var(-1.0) * self\n",
        "\n",
        "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return self + (-other)\n",
        "\n",
        "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
        "        return self * other ** -1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
        "\n",
        "    def relu(self):\n",
        "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\n",
        "    def identity(self):\n",
        "        return Var(self.v, lambda: [(self, 1.0)])\n",
        "    def tanh(self):\n",
        "        return Var(tanh(self.v), lambda: [(self, 1-self.v**2)])\n",
        "    def sigmoid(self): \n",
        "        return Var(self.v/(1+exp(-self.v)), lambda: [self,(1+exp(-self.v)+self.v*exp(-self.v))/(1+exp(-self.v))**2])\n",
        "    def exp(self):\n",
        "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
        "\n",
        "    def log(self):\n",
        "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDX67D6jzcte"
      },
      "source": [
        "A few examples illustrate how we can use this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk6PeLc3zwPT",
        "outputId": "52cefd53-e1d7-48aa-d96a-36ef437bf2d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=5.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(3.0)\n",
        "b = Var(5.0)\n",
        "f = a * b\n",
        "\n",
        "f.backward()\n",
        "\n",
        "for v in [a, b, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmKhYgsY0g_o",
        "outputId": "ad91d497-0226-4c76-e71e-b6a4e8276f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=14.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "Var(v=9.0000, grad=3.0000)\n",
            "Var(v=27.0000, grad=1.0000)\n",
            "Var(v=42.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(3.0)\n",
        "b = Var(5.0)\n",
        "c = a * b\n",
        "\n",
        "d = Var(9.0)\n",
        "e = a * d\n",
        "\n",
        "f = c + e\n",
        "\n",
        "f.backward()\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe3B6uEH140p"
      },
      "source": [
        "## Exercise a) What is being calculated?\n",
        "\n",
        "Explain briefly the output of the code? What is the expression we differentiate and with respect to what variables?\n",
        "\n",
        "The gradients and values of the floats are being calculated. The $\\textbf{Var}$ that has been defined not by others, would be in the inputlayer, if a $\\textbf{Var}$ depends on other $\\textbf{Var}$ it will be in a hiddenlayer.  The fact that \"a\" has a much higher gradiant means that it has a higher correlation with the loss function gradiant than b has. Using the chain rule we can evaluate the gradient for a paremeter $a$ as it was given in the second example as such: \n",
        "\n",
        "Example for gradiant of a: \n",
        "$$ \n",
        "\\frac{∂f}{∂a}=\\frac{\\partial f}{\\partial c} \\frac{\\partial c}{\\partial a} + \\frac{\\partial f}{\\partial e} \\frac{\\partial e}{\\partial a} = 1\\cdot9+1\\cdot5 = 14\n",
        "$$\n",
        "a has a high correlation with the output beacuse there are two chains containing a. This is also why we have a + sign in the equation above. \n",
        "Example of value of e: \n",
        "\n",
        "$$\n",
        "e = a.v \\cdot d.v = 3 \\cdot 9 = 27\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8_Q0t2I3Ruj"
      },
      "source": [
        "## Exercise b) How does the backward function work?\n",
        "\n",
        "You need to understand how the backward function calculates the gradients. We can use the two examples above to help with that.\n",
        "\n",
        "Go through the following four steps and answer the questions on the way:\n",
        "\n",
        "1. We represent the two expressions as graphs as shown below. Fill in the missing expressions for the different derivatives.\n",
        "\n",
        "2. In the remainder consider the first expression. Make a schematic of the data structure which is generated when we define the expression for f. \n",
        "\n",
        "\n",
        "#F is an instance of class Var that contains\n",
        "\n",
        "  - self.v = a.v *b.v = 15 \n",
        "  - self.grad_fn = [(a,b.v),(b,a.v)] = [(a,5) , (b,3)]\n",
        "  - self.grad = 1\n",
        "\n",
        "3. Then execute the backward function by hand to convince yourself that it indeed calculates the gradients with respect to the variables. - Done see examples of this in exercise a) \n",
        "\n",
        "4. Write down the sequence of calls to backprop.\n",
        "\n",
        "  - first expression:  f.backprop -> [i.backprop for i in f.grad_fn]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "idGr71jYXl26"
      },
      "outputs": [],
      "source": [
        "# import logging\n",
        "import graphviz\n",
        "\n",
        "#logging.basicConfig(format='[%(levelname)s@%(name)s] %(message)s', level=logging.DEBUG)\n",
        "\n",
        "#graphviz.__version__, graphviz.version()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "KPe30Q2QXzeG",
        "outputId": "ca0f0b34-55f8-407a-f4dc-1610c10108de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f4f78effd10>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: first expression Pages: 1 -->\n<svg width=\"162pt\" height=\"98pt\"\n viewBox=\"0.00 0.00 162.00 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n<title>first expression</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-94 158,-94 158,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-72\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a</text>\n</g>\n<!-- f -->\n<g id=\"node2\" class=\"node\">\n<title>f</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"136\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"136\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f</text>\n</g>\n<!-- a&#45;&gt;f -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.6658,-67.9578C54.8698,-63.5637 85.8151,-56.483 108.2661,-51.3459\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.2461,-54.7122 118.2134,-49.0698 107.6847,-47.8885 109.2461,-54.7122\"/>\n<text text-anchor=\"middle\" x=\"77\" y=\"-66.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/da=b</text>\n</g>\n<!-- b -->\n<g id=\"node3\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">b</text>\n</g>\n<!-- b&#45;&gt;f -->\n<g id=\"edge2\" class=\"edge\">\n<title>b&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M36.0339,-20.9637C52.7278,-23.8381 78.1872,-28.555 100,-34 103.0098,-34.7513 106.1413,-35.6022 109.2457,-36.4899\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"108.3051,-39.8618 118.8887,-39.3744 110.3112,-33.1554 108.3051,-39.8618\"/>\n<text text-anchor=\"middle\" x=\"77\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/db=a</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 236
        }
      ],
      "source": [
        "e1 = graphviz.Digraph('first expression', filename='fsm.gv')\n",
        "\n",
        "e1.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "e1.attr('node', shape='circle')\n",
        "e1.edge('a', 'f', label='df/da=b')\n",
        "e1.edge('b', 'f', label='df/db=a')\n",
        "\n",
        "e1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "0nittR-mZFeX",
        "outputId": "0b813c28-7c75-44cd-ac5a-fbe50108f71b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f4f79898190>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: second expression Pages: 1 -->\n<svg width=\"281pt\" height=\"158pt\"\n viewBox=\"0.00 0.00 281.00 158.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 154)\">\n<title>second expression</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-154 277,-154 277,4 -4,4\"/>\n<!-- a -->\n<g id=\"node1\" class=\"node\">\n<title>a</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-75\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">a</text>\n</g>\n<!-- c -->\n<g id=\"node2\" class=\"node\">\n<title>c</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"137\" cy=\"-102\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"137\" y=\"-98.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">c</text>\n</g>\n<!-- a&#45;&gt;c -->\n<g id=\"edge1\" class=\"edge\">\n<title>a&#45;&gt;c</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.5589,-78.9839C55.0696,-83.4108 86.8223,-90.6151 109.6075,-95.7849\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"108.8425,-99.2002 119.3691,-97.9997 110.3914,-92.3737 108.8425,-99.2002\"/>\n<text text-anchor=\"middle\" x=\"77.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dc/da=b</text>\n</g>\n<!-- e -->\n<g id=\"node4\" class=\"node\">\n<title>e</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"137\" cy=\"-48\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"137\" y=\"-44.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">e</text>\n</g>\n<!-- a&#45;&gt;e -->\n<g id=\"edge3\" class=\"edge\">\n<title>a&#45;&gt;e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.1093,-69.3663C41.0519,-67.518 47.7844,-65.5441 54,-64 72.2255,-59.4724 92.9698,-55.4733 109.1606,-52.6056\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.8139,-56.0446 119.0705,-50.8903 108.62,-49.1472 109.8139,-56.0446\"/>\n<text text-anchor=\"middle\" x=\"77.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">de/da=d</text>\n</g>\n<!-- f -->\n<g id=\"node6\" class=\"node\">\n<title>f</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"255\" cy=\"-75\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"255\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f</text>\n</g>\n<!-- c&#45;&gt;f -->\n<g id=\"edge5\" class=\"edge\">\n<title>c&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M154.6658,-97.9578C173.8698,-93.5637 204.8151,-86.483 227.2661,-81.3459\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"228.2461,-84.7122 237.2134,-79.0698 226.6847,-77.8885 228.2461,-84.7122\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/dc=1</text>\n</g>\n<!-- b -->\n<g id=\"node3\" class=\"node\">\n<title>b</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-132\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-128.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">b</text>\n</g>\n<!-- b&#45;&gt;c -->\n<g id=\"edge2\" class=\"edge\">\n<title>b&#45;&gt;c</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.5589,-127.5734C55.0696,-122.6547 86.8223,-114.6498 109.6075,-108.9057\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.5281,-112.2832 119.3691,-106.4448 108.8169,-105.4955 110.5281,-112.2832\"/>\n<text text-anchor=\"middle\" x=\"77.5\" y=\"-125.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dc/db=a</text>\n</g>\n<!-- e&#45;&gt;f -->\n<g id=\"edge6\" class=\"edge\">\n<title>e&#45;&gt;f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M155.0339,-50.9637C171.7278,-53.8381 197.1872,-58.555 219,-64 222.0098,-64.7513 225.1413,-65.6022 228.2457,-66.4899\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"227.3051,-69.8618 237.8887,-69.3744 229.3112,-63.1554 227.3051,-69.8618\"/>\n<text text-anchor=\"middle\" x=\"196\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">df/de=1</text>\n</g>\n<!-- d -->\n<g id=\"node5\" class=\"node\">\n<title>d</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">d</text>\n</g>\n<!-- d&#45;&gt;e -->\n<g id=\"edge4\" class=\"edge\">\n<title>d&#45;&gt;e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.9889,-20.9104C52.9226,-23.8318 78.9119,-28.785 101,-35 104.0364,-35.8544 107.1845,-36.8389 110.2985,-37.8758\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.3565,-41.2547 119.9514,-41.2723 111.68,-34.6515 109.3565,-41.2547\"/>\n<text text-anchor=\"middle\" x=\"77.5\" y=\"-38.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">de/dd=a</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 237
        }
      ],
      "source": [
        "e2 = graphviz.Digraph('second expression', filename='fsm.gv')\n",
        "\n",
        "e2.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "e2.attr('node', shape='circle')\n",
        "e2.edge('a', 'c', label='dc/da=b')\n",
        "e2.edge('b', 'c', label='dc/db=a')\n",
        "e2.edge('a', 'e', label='de/da=d')\n",
        "e2.edge('d', 'e', label='de/dd=a')\n",
        "e2.edge('c', 'f', label='df/dc=1')\n",
        "e2.edge('e', 'f', label='df/de=1')\n",
        "\n",
        "e2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5oi21W4gpeM"
      },
      "source": [
        "## Exercise c) What happens if we run backward again?\n",
        "\n",
        "Try to execute the code below. Explain what happens.\n",
        "\n",
        "First f.backprop is called oneadding the evaluated gradiants to all Var.grad in f.grad_fn. Second time f.backprop is called all Var in f.grad_fn will have the evaluated gradians added one more time to the attribute Var.grad. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCtpJyr-gyX1",
        "outputId": "99c74c1a-bfcd-450a-eef5-30bb4fb032a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=3.0000, grad=28.0000)\n",
            "Var(v=5.0000, grad=6.0000)\n",
            "Var(v=15.0000, grad=2.0000)\n",
            "Var(v=9.0000, grad=6.0000)\n",
            "Var(v=27.0000, grad=2.0000)\n",
            "Var(v=42.0000, grad=2.0000)\n"
          ]
        }
      ],
      "source": [
        "f.backward()\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8bPVq2VhsP-"
      },
      "source": [
        "## Exercise d) Zero gradient\n",
        "\n",
        "We can zero the gradient by backpropagating a -1.0 as is shown in the example below. (If you have run backward multiple time then you also have to run the cell below an equal amount of times.) Explain what is going on.\n",
        "\n",
        "#Answer\n",
        "\n",
        "When redefining a as a instance of class Var, the attribute a.grad is by default 0.0 and if only changed if backwards og backprop functions are changed. (or if its manually defined) \n",
        "\n",
        "when calling f.backwards() it calls the function backprop(1). This means that the evaluated gradiants will be multiplied with 1 before getting added to the attribute Var.grad. If f.backprop(-1) is called the evaluated gradiants will be multiplied with -1 before getting added to the attribute Var.grad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnyPDQx9lJe0",
        "outputId": "49f9ac73-d963-4904-f11e-e1b4066cc58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=2.0000, grad=0.0000)\n",
            "Var(v=5.0000, grad=6.0000)\n",
            "Var(v=15.0000, grad=2.0000)\n",
            "Var(v=9.0000, grad=6.0000)\n",
            "Var(v=27.0000, grad=2.0000)\n",
            "Var(v=42.0000, grad=2.0000)\n",
            "\n",
            "Var(v=2.0000, grad=0.0000)\n",
            "Var(v=5.0000, grad=3.0000)\n",
            "Var(v=15.0000, grad=1.0000)\n",
            "Var(v=9.0000, grad=3.0000)\n",
            "Var(v=27.0000, grad=1.0000)\n",
            "Var(v=42.0000, grad=1.0000)\n"
          ]
        }
      ],
      "source": [
        "a = Var(2.0)\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)\n",
        "print()\n",
        "f.backprop(-1.0)\n",
        "\n",
        "for v in [a, b, c, d, e, f]:\n",
        "    print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4057_ljNvWB"
      },
      "source": [
        "## Exercise e) Test correctness of derivatives with the finite difference method\n",
        "\n",
        "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to numerically test that backpropation implementation is working. In short we will use\n",
        "$$\n",
        "\\frac{\\partial f(a)}{\\partial a} \\approx \\frac{f(a+da)-f(a)}{da}\n",
        "$$\n",
        "for $da \\ll 1$.\n",
        "\n",
        "As an example, we could approximate the derivative of the function $f(a)=a^2$ in e.g. the value $a=4$ using the finite difference method. This amounts to inserting the relevant values and approximating the gradient $f'(4)$ with the fraction above. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TGil92lSXDN",
        "outputId": "f4b89868-0868-47b5-f5ca-cf4b6b68e3f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Var(v=4.0000, grad=5.0000)\n",
            "Var(v=5.0000, grad=4.0000)\n",
            "Var(v=20.0000, grad=1.0000)\n",
            "finitemethod: 8.000000661922968\n",
            "actual: 8\n"
          ]
        }
      ],
      "source": [
        "# f function - try to change the code to test other types of functions as well (such as different polynomials etc.)\n",
        "def f_function(a):\n",
        "  a = Var(a)\n",
        "  b = Var(5.0)\n",
        "  f = a * b\n",
        "  f.backward()\n",
        "  return a,b,f\n",
        "\n",
        "for v in f_function(4.0):\n",
        "  print(v)\n",
        "\n",
        "def pol_eval(a):\n",
        "  return a**2\n",
        "\n",
        "\n",
        "# Insert your finite difference code here\n",
        "def finite_difference(da=1e-10):\n",
        "    \"\"\"\n",
        "    This function compute the finite difference between\n",
        "    \n",
        "    Input:\n",
        "    da:          The finite difference                           (float)\n",
        "    \n",
        "    Output:\n",
        "    finite_difference: numerical approximation to the derivative (float) \n",
        "    \"\"\"\n",
        "    \n",
        "    fa_da = pol_eval(4+da)           # <- Insert correct expression\n",
        "    fa = pol_eval(4)               # <- Insert correct expression\n",
        "\n",
        "    finite_difference = (fa_da - fa) / da\n",
        "    \n",
        "    return finite_difference\n",
        "\n",
        "print(\"finitemethod:\",finite_difference())\n",
        "print(\"actual:\" ,8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZar5RKaUkg"
      },
      "source": [
        "# Create an artificial dataset to play with\n",
        "\n",
        "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "Y6yfMAQ8aduj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "4YabfD43ajNh"
      },
      "outputs": [],
      "source": [
        "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
        "    # Create covariates and response variable\n",
        "    if D1:\n",
        "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
        "        np.random.shuffle(X)\n",
        "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
        "    else:\n",
        "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
        "        np.random.shuffle(X)    \n",
        "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
        "\n",
        "    # Stack them together vertically to split data set\n",
        "    data_set = np.vstack((X.T,y)).T\n",
        "    \n",
        "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
        "    \n",
        "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
        "    train_mu = np.mean(train, axis=0)\n",
        "    train_sigma = np.std(train, axis=0)\n",
        "    \n",
        "    train = (train-train_mu)/train_sigma\n",
        "    validation = (validation-train_mu)/train_sigma\n",
        "    test = (test-train_mu)/train_sigma\n",
        "    \n",
        "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
        "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
        "\n",
        "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "u1oDngHLapIz"
      },
      "outputs": [],
      "source": [
        "D1 = True\n",
        "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Ysfa3FsBavlm",
        "outputId": "b3198864-0253-4d5c-e01d-9c629bf33376"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "if D1:\n",
        "    plt.scatter(x_train[:,0], y_train);\n",
        "    plt.scatter(x_validation[:,0], y_validation);\n",
        "    plt.scatter(x_test[:,0], y_test);\n",
        "else:\n",
        "    plt.scatter(x_train[:,1], y_train);\n",
        "    plt.scatter(x_validation[:,1], y_validation);\n",
        "    plt.scatter(x_test[:,1], y_test);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "zac2HHNlgbpm"
      },
      "outputs": [],
      "source": [
        "# convert from nparray to Var\n",
        "def nparray_to_Var(x):\n",
        "  if x.ndim==1:\n",
        "    y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\n",
        "  else:\n",
        "    y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n",
        "  return y\n",
        "   \n",
        "x_train = nparray_to_Var(x_train)\n",
        "y_train = nparray_to_Var(y_train)\n",
        "x_validation = nparray_to_Var(x_validation)\n",
        "y_validation = nparray_to_Var(y_validation)\n",
        "x_test = nparray_to_Var(x_test)\n",
        "y_test = nparray_to_Var(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbjrqcpVFtGe"
      },
      "source": [
        "# Defining and initializing the network\n",
        "\n",
        "The steps to create a feed forward neural network are the following:\n",
        "\n",
        "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. The number of features in X and the output dimensionality (the size of Y) are given but the numbers in between are set by the researcher. Remember that for each unit in each layer beside in the input has a bias term.\n",
        "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
        "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
        "\n",
        "In order to make life easier for ourselves we define a DenseLayer class that takes care of initialization and the forward pass. We can also extend it later with print and advanced initialization capabilities. For the latter we have introduced a Initializer class.\n",
        "\n",
        "Note that we use Sequence in the code below. A Sequence is an ordered list. This means the order we insert and access items are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "ij_ieRsAt7Xt"
      },
      "outputs": [],
      "source": [
        "class Initializer:\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "eb18N5phuIha"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class NormalInitializer(Initializer):\n",
        "\n",
        "  def __init__(self, mean=0, std=0.1):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    return [Var(0.0) for _ in range(n_out)]\n",
        "\n",
        "class ConstantInitializer(Initializer):\n",
        "\n",
        "  def __init__(self, weight=1.0, bias=0.0):\n",
        "    self.weight = weight\n",
        "    self.bias = bias\n",
        "\n",
        "  def init_weights(self, n_in, n_out):\n",
        "    return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\n",
        "\n",
        "  def init_bias(self, n_out):\n",
        "    return [Var(self.bias) for _ in range(n_out)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "jOLYGnZKuM6W"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer()):\n",
        "        self.weights = initializer.init_weights(n_in, n_out)\n",
        "        self.bias = initializer.init_bias(n_out)\n",
        "        self.act_fn = act_fn\n",
        "    \n",
        "    def __repr__(self):    \n",
        "        return 'Weights: ' + repr(self.weights) + ' Biases: ' + repr(self.bias)\n",
        "\n",
        "    def parameters(self) -> Sequence[Var]:\n",
        "      params = []\n",
        "      for r in self.weights:\n",
        "        params += r\n",
        "\n",
        "      return params + self.bias\n",
        "\n",
        "    def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
        "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input \n",
        "        # to the current layer matches the number of nodes in the current layer\n",
        "        assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
        "        weights = self.weights\n",
        "        bias = self.bias\n",
        "        out = []\n",
        "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
        "        # We therefore loop over the (number of) nodes in the current layer:\n",
        "        for j in range(len(weights[0])): \n",
        "            # Initialize the node value depending on its corresponding parameters.\n",
        "            node =  bias[j]\n",
        "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
        "            for i in range(len(single_input)):\n",
        "                node += weights[i][j]*single_input[i]\n",
        "            node = self.act_fn(node)\n",
        "            out.append(node)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIZPBpNI0pO"
      },
      "source": [
        "## Exercise f) Add more activation functions\n",
        "\n",
        "To have a full definition of the neural network, we must define an activation function for every layer. Several activation functions have been proposed and have different characteristics. In the Var class we have already defined the rectified linear init (relu). \n",
        " \n",
        "Implement the following activation functions in the Var class:\n",
        "\n",
        "* Identity: $$\\mathrm{identity}(x) = x$$\n",
        "* Hyperbolic tangent: $$\\tanh(x)$$\n",
        "* Sigmoid (or logistic function): $$\\mathrm{sigmoid}(x) = \\frac{1}{1.0 + \\exp(-x ) }$$  Hint: $\\mathrm{sigmoid}'(x)= \\mathrm{sigmoid}(x)(1-\\mathrm{sigmoid}(x))$.  \n",
        "\n",
        "Hint: You can seek inspiration in the relu method in the Var class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_8n_SKnIW2F"
      },
      "source": [
        "## Exercise g) Complete the forward pass\n",
        "\n",
        "In the code below we initialize a 1-5-1 network and pass the training set through it. *The forward method in DenseLayer is **not** complete*. It just outputs zeros right now. The method forward should perform an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) on the input followed by an application of the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "xDEjtePxE7Mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b67685e-f7d6-4bea-f339-46361ab195b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[Var(v=0.0072, grad=0.0000)], [Var(v=0.0145, grad=0.0000)], [Var(v=0.0013, grad=0.0000)], [Var(v=-0.0044, grad=0.0000)], [Var(v=0.0107, grad=0.0000)], [Var(v=0.0099, grad=0.0000)], [Var(v=0.0064, grad=0.0000)], [Var(v=-0.0011, grad=0.0000)], [Var(v=-0.0046, grad=0.0000)], [Var(v=0.0040, grad=0.0000)], [Var(v=0.0111, grad=0.0000)], [Var(v=-0.0028, grad=0.0000)], [Var(v=0.0089, grad=0.0000)], [Var(v=-0.0032, grad=0.0000)], [Var(v=0.0048, grad=0.0000)], [Var(v=0.0093, grad=0.0000)], [Var(v=0.0171, grad=0.0000)], [Var(v=0.0081, grad=0.0000)], [Var(v=0.0009, grad=0.0000)], [Var(v=0.0028, grad=0.0000)], [Var(v=-0.0021, grad=0.0000)], [Var(v=-0.0009, grad=0.0000)], [Var(v=0.0125, grad=0.0000)], [Var(v=0.0126, grad=0.0000)], [Var(v=-0.0012, grad=0.0000)], [Var(v=-0.0033, grad=0.0000)], [Var(v=0.0162, grad=0.0000)], [Var(v=0.0178, grad=0.0000)], [Var(v=0.0019, grad=0.0000)], [Var(v=0.0113, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=0.0027, grad=0.0000)], [Var(v=-0.0036, grad=0.0000)], [Var(v=-0.0039, grad=0.0000)], [Var(v=0.0086, grad=0.0000)], [Var(v=-0.0007, grad=0.0000)], [Var(v=-0.0045, grad=0.0000)], [Var(v=-0.0017, grad=0.0000)], [Var(v=-0.0032, grad=0.0000)], [Var(v=-0.0023, grad=0.0000)], [Var(v=-0.0016, grad=0.0000)], [Var(v=-0.0022, grad=0.0000)], [Var(v=0.0168, grad=0.0000)], [Var(v=-0.0027, grad=0.0000)], [Var(v=-0.0021, grad=0.0000)], [Var(v=-0.0026, grad=0.0000)], [Var(v=0.0108, grad=0.0000)], [Var(v=0.0102, grad=0.0000)], [Var(v=-0.0010, grad=0.0000)], [Var(v=0.0104, grad=0.0000)], [Var(v=0.0045, grad=0.0000)], [Var(v=0.0004, grad=0.0000)], [Var(v=0.0114, grad=0.0000)], [Var(v=-0.0022, grad=0.0000)], [Var(v=0.0181, grad=0.0000)], [Var(v=0.0159, grad=0.0000)], [Var(v=-0.0015, grad=0.0000)], [Var(v=-0.0016, grad=0.0000)], [Var(v=0.0059, grad=0.0000)], [Var(v=-0.0039, grad=0.0000)], [Var(v=0.0106, grad=0.0000)], [Var(v=-0.0028, grad=0.0000)], [Var(v=-0.0045, grad=0.0000)], [Var(v=0.0051, grad=0.0000)], [Var(v=0.0037, grad=0.0000)], [Var(v=-0.0037, grad=0.0000)], [Var(v=-0.0040, grad=0.0000)], [Var(v=0.0133, grad=0.0000)], [Var(v=-0.0029, grad=0.0000)], [Var(v=0.0052, grad=0.0000)], [Var(v=-0.0011, grad=0.0000)], [Var(v=-0.0005, grad=0.0000)], [Var(v=0.0103, grad=0.0000)], [Var(v=0.0061, grad=0.0000)], [Var(v=-0.0004, grad=0.0000)], [Var(v=0.0163, grad=0.0000)], [Var(v=-0.0020, grad=0.0000)], [Var(v=-0.0019, grad=0.0000)], [Var(v=-0.0025, grad=0.0000)], [Var(v=0.0170, grad=0.0000)], [Var(v=0.0026, grad=0.0000)], [Var(v=0.0015, grad=0.0000)], [Var(v=-0.0041, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0021, grad=0.0000)], [Var(v=-0.0009, grad=0.0000)], [Var(v=-0.0008, grad=0.0000)], [Var(v=-0.0023, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=-0.0044, grad=0.0000)], [Var(v=0.0062, grad=0.0000)], [Var(v=-0.0013, grad=0.0000)], [Var(v=-0.0024, grad=0.0000)], [Var(v=0.0096, grad=0.0000)], [Var(v=-0.0035, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=0.0044, grad=0.0000)], [Var(v=0.0007, grad=0.0000)], [Var(v=0.0154, grad=0.0000)], [Var(v=-0.0025, grad=0.0000)], [Var(v=0.0101, grad=0.0000)], [Var(v=-0.0024, grad=0.0000)], [Var(v=-0.0037, grad=0.0000)], [Var(v=0.0049, grad=0.0000)], [Var(v=0.0144, grad=0.0000)]]\n"
          ]
        }
      ],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 5, lambda x: x.relu()),\n",
        "    DenseLayer(5, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "def forward(input, network):\n",
        "\n",
        "  def forward_single(x, network):\n",
        "    for layer in network:\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  output = [ forward_single(input[n], network) for n in range(len(input))]\n",
        "  return output\n",
        "\n",
        "print(forward(x_train, NN))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrGJytZFtGm"
      },
      "source": [
        "## Exercise h) Print all network parameters\n",
        "\n",
        "Make a function that prints all the parameters of the network (weights and biases) with information about in which layer the appear. In the object oriented spirit you should introduce a method in the DenseLayer class to print the parameters of a layer. Hint: You can take inspiration from the corresponding method in Var. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "iac-VwYGFtGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcaebb7-35c3-46b6-e6d8-e5c0d407c583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer 1 Weights: [[Var(v=-0.0469, grad=0.0000), Var(v=-0.0216, grad=0.0000), Var(v=-0.0082, grad=0.0000), Var(v=0.0649, grad=0.0000), Var(v=0.0579, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
            "layer 2 Weights: [[Var(v=-0.0794, grad=0.0000)], [Var(v=0.0429, grad=0.0000)], [Var(v=-0.0148, grad=0.0000)], [Var(v=0.1338, grad=0.0000)], [Var(v=0.0226, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n"
          ]
        }
      ],
      "source": [
        "# Insert code here and in the DenseLayer class\n",
        "def print_parameters(network):\n",
        "  for i,layer in enumerate(network):\n",
        "    print(f\"layer {i+1}\" ,layer)\n",
        "\n",
        "\n",
        "print_parameters(NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79HOAXrFtHK"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Now that we have defined our activation functions we can visualize them to see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "1FcylHqLTl-Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "595b65c8-b829-4836-a480-619e35398ac3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4f7e7f0990>]"
            ]
          },
          "metadata": {},
          "execution_count": 251
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x = np.linspace(-6, 6, 100)\n",
        "\n",
        "# convert from Var to ndarray  \n",
        "def Var_to_nparray(x):\n",
        "  y = np.zeros((len(x),len(x[0])))\n",
        "  for i in range(len(x)):\n",
        "    for j in range(len(x[0])):\n",
        "      y[i,j] = x[i][j].v\n",
        "  return y\n",
        "\n",
        "# define 1-1 network with weight = 1 and relu activation \n",
        "NN = [ DenseLayer(1, 1, lambda x: x.relu(), initializer = ConstantInitializer(1.0)) ] \n",
        "y = Var_to_nparray(forward(nparray_to_Var(x), NN))\n",
        "\n",
        "#y = Var_to_nparray(relu(nparray_to_Var(x)))\n",
        "plt.plot(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "oOL2UolJFtHL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "39ddd9e7-b3ae-4470-da59-ace44beef500"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Testing all activation layers\n",
        "\n",
        "x = np.linspace(-6, 6, 100)\n",
        "units = {\n",
        "    \"identity\": lambda x: x.identity(),\n",
        "    #\"sigmoid\": lambda x: x.sigmoid(),  <- uncomment before sharing\n",
        "    \"relu\": lambda x: x.relu(),\n",
        "    #\"tanh\": lambda x: x.tanh() <- uncomment before sharing\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "[plt.plot(x, Var_to_nparray(forward(nparray_to_Var(x), [DenseLayer(1, 1, unit, initializer = ConstantInitializer(1.0))]) ), label=unit_name, lw=2) for unit_name, unit in units.items()] # unit(nparray_to_Var(x))), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
        "plt.legend(loc=2, fontsize=16)\n",
        "plt.title('Our activation functions', fontsize=20)\n",
        "plt.ylim([-2, 5])\n",
        "plt.xlim([-6, 6])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jdEl-7FtGs"
      },
      "source": [
        "# Advanced initialization schemes\n",
        "\n",
        "If we are not careful with initialization, the signals we propagate forward ($a^{(l)}$, $l=1,\\ldots,L$) and backward ($\\delta^l$, $l=L,L-1,\\ldots,1$) can blow up or shrink to zero. A statistical analysis of the variance of the signals for different activation functions can be found in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
        "\n",
        "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept approxmimatly constant when propagating from layer to layer. The exact expressions depend upon the non-linear activation function used. In Glorot initialization, the aim is to keep both the forward and backward variances constant whereas He only aims at keeping the variance in the forward pass constant.\n",
        "\n",
        "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
        "\n",
        "The Glorot initialization has the form: \n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2 \\alpha }{n_{in} + n_{out}} \\bigg) \\ . $$\n",
        "\n",
        "where $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\alpha$ is a parameter that depends upon the activation function used. For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$. (It is also possible to use a uniform distribution for initialization, see [this blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init).) \n",
        "\n",
        "The He initialization is very similar\n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{\\alpha}{n_{in}} \\bigg) \\ . $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqeyab9qFtGs"
      },
      "source": [
        "## Exercise i) Glorot and He initialization\n",
        " \n",
        "Using the Initializer class, implement functions that implement Glorot and He \n",
        "\n",
        "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper.\n",
        "\n",
        "Comment: If you want to be more advanced then try to make a universal initializer taking both the activation function and type (Glorot or He) as argument.\n",
        "\n",
        "##Answer:\n",
        "\n",
        "Testing whether the signal values for both implementated networks, will result in a stable or unstable function. The function is unstable (i.e. goes to infinity for relu and identity) or goes infinitly close to zero or one for the other activation funtions. This can be testet numerically. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "Qyk01CgaFtGt"
      },
      "outputs": [],
      "source": [
        "## Glorot\n",
        "def DenseLayer_Glorot_tanh(n_in: int, n_out: int):\n",
        "  std = np.sqrt((2*a)/(n_in+n_out)) # <- replace with proper initialization\n",
        "  return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\n",
        "\n",
        "## He\n",
        "def DenseLayer_He_relu(n_in: int, n_out: int):\n",
        "  std = np.sqrt( a/n_in) # <- replace with proper initialization\n",
        "  return DenseLayer(n_in, n_out, lambda x: x.relu(), initializer = NormalInitializer(std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyXBD37FtHk"
      },
      "source": [
        "## Exercise j) Forward pass unit test\n",
        "\n",
        "Write a bit of code to make a unit test that the forward pass works. This can be done by defining a simple network with for example all weights equal to one (using the ConstantInitializer method) and identity activation functions. \n",
        "\n",
        "Hints: Use the [assert](https://www.w3schools.com/python/ref_keyword_assert.asp), the nparray_to_Var and the Var_to_nparray commands. \n",
        "\n",
        "#Answer\n",
        "\n",
        "The simple network is made with all weights = 1 and bias = 0 with 1 intput and a layer of 2 nodes and 1 output. This results in all outputs to be 2 times the input. Hence the assert: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "k0miqRUAFtHl"
      },
      "outputs": [],
      "source": [
        "# Insert code here\n",
        "x = np.asarray([i for i in range(5)])\n",
        "x = nparray_to_Var(x)\n",
        "NN = [\n",
        "    DenseLayer(1, 2, lambda x: x.identity(),initializer= ConstantInitializer()),\n",
        "    DenseLayer(2, 1, lambda x: x.identity(),initializer= ConstantInitializer())\n",
        "]\n",
        "result = Var_to_nparray(forward(x,NN))\n",
        "\n",
        "for i,val in enumerate(result):\n",
        "    assert (val == [i*2])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faCxhfFnFtHp"
      },
      "source": [
        "\n",
        "# Loss functions\n",
        "\n",
        "We are only missing a loss function to we need to define a loss function and its derivative with respect to the output of the neural network $y$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "I2eDYKvAFtHq"
      },
      "outputs": [],
      "source": [
        "def squared_loss(t, y):\n",
        "  \n",
        "  # add check that sizes agree\n",
        "  assert(len(t) == len(y))\n",
        "  \n",
        "  def squared_loss_single(t, y):\n",
        "    Loss = Var(0.0)\n",
        "    for i in range(len(t)): # sum over outputs\n",
        "      Loss += (t[i]-y[i]) ** 2\n",
        "    return Loss\n",
        "\n",
        "  Loss = Var(0.0)\n",
        "  for n in range(len(t)): # sum over training data\n",
        "    Loss += squared_loss_single(t[n],y[n])\n",
        "  return Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrwSJ2UWFtHu"
      },
      "source": [
        "## Exercise j) Implement cross entropy loss\n",
        "\n",
        "Insert code below to implement cross-entropy loss for general dimensionality of $t$. Use a logits formulation:\n",
        "$$\n",
        "\\rm{Loss} = - \\sum_i t_i \\, log \\, p_i \n",
        "$$\n",
        "with $p$ given by the the softmax function in terms of the logits $h$:\n",
        "$$\n",
        "p_i = \\frac{\\exp(h_i)}{\\sum_{i'} \\exp(h_{i'})} .\n",
        "$$\n",
        "Inserting $p$ in the expression for the loss gives\n",
        "$$\n",
        "\\rm{Loss} = - \\sum_i t_i h_i + \\rm{LogSumExp}(h) \\ ,\n",
        "$$\n",
        "where \n",
        "$$\n",
        "\\rm{LogSumExp}(h) = \\log \\sum_i \\exp h_i \\ .\n",
        "$$\n",
        "This is true for $t$ being a one-hot vector. \n",
        "\n",
        "Call the function to convince yourself it works. \n",
        "\n",
        "In practice you want to implement a [numerically stable](https://leimao.github.io/blog/LogSumExp/) version of LogSumExp. But we will not bother about that here.\n",
        "\n",
        "Help: You can add these methods in the Var class:\n",
        "\n",
        "    def exp(self):\n",
        "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
        "    \n",
        "    def log(self):\n",
        "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "6nMuxyfzFtHv"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(t, h):\n",
        "  assert ( len(t) == len(h))\n",
        "\n",
        "  def cross_entropy_loss_single(t,h):\n",
        "    Loss = Var(0.0)\n",
        "    logsum = Var(0.0)\n",
        "    # Insert code here\n",
        "    for i,t_val in enumerate(t):\n",
        "      Loss += -t_val*h[i]\n",
        "      logsum += h[i].exp()\n",
        "\n",
        "    Loss += logsum.log() \n",
        "    return Loss\n",
        "  \n",
        "  Loss = Var(0.0)\n",
        "  for n in range(len(t)): # sum over training data\n",
        "    Loss += cross_entropy_loss_single(t[n],y[n])\n",
        "  return Loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAF5ew4FtHy"
      },
      "source": [
        "# Backward pass\n",
        "\n",
        "Now the magic happens! We get the calculation of the gradients for free. Just do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "iHyfPPI9Qqwu"
      },
      "outputs": [],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 5, lambda x: x.relu()),\n",
        "    DenseLayer(5, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "output = forward(x_train, NN)\n",
        "\n",
        "Loss = squared_loss(y_train,output)\n",
        "Loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49biIAYKQ1oG"
      },
      "source": [
        "and the gradients will be calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "_rGt1bq_Q7uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5075f86-8b79-4f31-ecca-93a05cf10738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 \n",
            " Weights: [[Var(v=-0.1991, grad=11.0752), Var(v=-0.0493, grad=-10.2533), Var(v=-0.1873, grad=16.8724), Var(v=-0.0309, grad=-7.6482), Var(v=-0.1756, grad=-1.4835)]] Biases: [Var(v=0.0000, grad=-10.0575), Var(v=0.0000, grad=9.3112), Var(v=0.0000, grad=-15.3221), Var(v=0.0000, grad=6.9454), Var(v=0.0000, grad=1.3472)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.1214, grad=18.1597)], [Var(v=0.1124, grad=4.4994)], [Var(v=-0.1850, grad=17.0860)], [Var(v=0.0838, grad=2.8211)], [Var(v=0.0163, grad=16.0152)]] Biases: [Var(v=0.0000, grad=-4.4288)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ],
      "source": [
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d7qK0uFtH9"
      },
      "source": [
        "# Backward pass unit test\n",
        "\n",
        "Above we used finite differences to test that Nanograd is actually doing what it is supposed to do. We can in principle try the same for the neural network. But we will trust that the test above is enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgBi8GOSFtIN"
      },
      "source": [
        "# Training and validation\n",
        "\n",
        "We are ready to train some neural networks!\n",
        "\n",
        "We initialize again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "01ePmzBzRtdh"
      },
      "outputs": [],
      "source": [
        "NN = [\n",
        "    DenseLayer(1, 15, lambda x: x.relu()),\n",
        "    DenseLayer(15, 50, lambda x: x.relu()),\n",
        "    DenseLayer(50, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "output = forward(x_train, NN)\n",
        "\n",
        "Loss = squared_loss(y_train,output)\n",
        "Loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10iRPiQ1ISHw"
      },
      "source": [
        "and make an update:\n",
        "\n",
        "We introduce a help function parameters to have a handle in all parameters in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "dhAI7eyeznia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93b747c-aa00-4037-d3c5-e64a97373ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network before update:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=-0.0033, grad=1.3311), Var(v=-0.1285, grad=-0.2807), Var(v=-0.0677, grad=8.3540), Var(v=0.0110, grad=3.4471), Var(v=-0.0334, grad=7.9912), Var(v=0.0139, grad=0.3893), Var(v=-0.0107, grad=-2.4088), Var(v=-0.0329, grad=-1.5518), Var(v=-0.1887, grad=5.8693), Var(v=-0.0632, grad=-14.1517), Var(v=0.1608, grad=7.0739), Var(v=-0.1165, grad=2.0652), Var(v=-0.0936, grad=4.9511), Var(v=0.0451, grad=-11.9045), Var(v=-0.0353, grad=3.4011)]] Biases: [Var(v=0.0000, grad=-1.2095), Var(v=0.0000, grad=0.2551), Var(v=0.0000, grad=-7.5907), Var(v=0.0000, grad=2.9607), Var(v=0.0000, grad=-7.2611), Var(v=0.0000, grad=0.3344), Var(v=0.0000, grad=2.1887), Var(v=0.0000, grad=1.4101), Var(v=0.0000, grad=-5.3330), Var(v=0.0000, grad=12.8587), Var(v=0.0000, grad=6.0757), Var(v=0.0000, grad=-1.8765), Var(v=0.0000, grad=-4.4988), Var(v=0.0000, grad=-10.2247), Var(v=0.0000, grad=-3.0903)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.0343, grad=-0.0176), Var(v=0.2596, grad=0.0000), Var(v=0.0329, grad=0.0209), Var(v=0.0797, grad=-0.0808), Var(v=0.1167, grad=0.0000), Var(v=0.1777, grad=0.0000), Var(v=-0.1692, grad=0.0133), Var(v=-0.0884, grad=-0.0322), Var(v=-0.0355, grad=0.0000), Var(v=-0.1078, grad=0.0160), Var(v=0.1216, grad=0.0025), Var(v=-0.1300, grad=0.0000), Var(v=0.0500, grad=0.0000), Var(v=-0.0657, grad=0.0000), Var(v=-0.1067, grad=0.0000), Var(v=0.0295, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.2480, grad=0.0254), Var(v=0.1019, grad=0.0077), Var(v=0.0111, grad=0.0000), Var(v=-0.1020, grad=0.0000), Var(v=-0.0073, grad=0.0430), Var(v=-0.0215, grad=0.0006), Var(v=-0.0346, grad=0.0000), Var(v=-0.0588, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=-0.0412, grad=0.0000), Var(v=-0.0345, grad=0.0000), Var(v=-0.0248, grad=0.0000), Var(v=-0.0763, grad=-0.0643), Var(v=-0.0285, grad=0.0458), Var(v=-0.1563, grad=0.0000), Var(v=0.0072, grad=0.0000), Var(v=-0.1272, grad=0.0148), Var(v=0.0047, grad=0.0000), Var(v=0.3266, grad=-0.0086), Var(v=-0.0209, grad=-0.0428), Var(v=0.1360, grad=0.0000), Var(v=-0.1031, grad=0.0000), Var(v=-0.0361, grad=0.0000), Var(v=-0.1102, grad=0.0000), Var(v=-0.0052, grad=0.0000), Var(v=-0.0487, grad=0.0092), Var(v=0.0107, grad=0.0098), Var(v=0.0507, grad=0.0000), Var(v=-0.1748, grad=0.0000), Var(v=-0.0914, grad=-0.0435), Var(v=0.0415, grad=-0.0024), Var(v=0.0620, grad=-0.0291), Var(v=-0.1867, grad=-0.0316)], [Var(v=-0.0794, grad=-0.6899), Var(v=-0.0818, grad=0.0000), Var(v=-0.0344, grad=0.8213), Var(v=-0.0019, grad=-3.1726), Var(v=-0.2672, grad=0.0000), Var(v=-0.0861, grad=0.0000), Var(v=0.0700, grad=0.5228), Var(v=0.0220, grad=-1.2653), Var(v=-0.1109, grad=0.0000), Var(v=-0.0304, grad=0.6302), Var(v=0.0442, grad=0.0995), Var(v=0.0006, grad=0.0000), Var(v=-0.0567, grad=0.0000), Var(v=-0.0825, grad=0.0000), Var(v=-0.1025, grad=0.0000), Var(v=0.0132, grad=0.0000), Var(v=-0.0494, grad=0.0000), Var(v=0.1157, grad=0.9967), Var(v=0.1472, grad=0.3026), Var(v=-0.0520, grad=0.0000), Var(v=-0.1259, grad=0.0000), Var(v=0.0507, grad=1.6909), Var(v=0.0810, grad=0.0244), Var(v=0.0061, grad=0.0000), Var(v=-0.0026, grad=0.0000), Var(v=-0.0059, grad=0.0000), Var(v=0.1792, grad=0.0000), Var(v=0.0162, grad=0.0000), Var(v=-0.0467, grad=0.0000), Var(v=0.0553, grad=-2.5256), Var(v=0.0167, grad=1.8000), Var(v=-0.1821, grad=0.0000), Var(v=-0.0426, grad=0.0000), Var(v=-0.1438, grad=0.5798), Var(v=-0.1210, grad=0.0000), Var(v=-0.0872, grad=-0.3366), Var(v=0.0379, grad=-1.6797), Var(v=0.0778, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=0.0102, grad=0.0000), Var(v=0.1392, grad=0.0000), Var(v=-0.0149, grad=0.0000), Var(v=-0.0288, grad=0.3614), Var(v=0.2660, grad=0.3858), Var(v=-0.0205, grad=0.0000), Var(v=0.0265, grad=0.0000), Var(v=0.0293, grad=-1.7093), Var(v=-0.1452, grad=-0.0929), Var(v=0.0025, grad=-1.1431), Var(v=0.0511, grad=-1.2425)], [Var(v=0.0332, grad=-0.3634), Var(v=-0.0234, grad=0.0000), Var(v=0.0044, grad=0.4327), Var(v=0.0055, grad=-1.6712), Var(v=-0.0649, grad=0.0000), Var(v=-0.0680, grad=0.0000), Var(v=0.0020, grad=0.2754), Var(v=0.1766, grad=-0.6665), Var(v=0.0335, grad=0.0000), Var(v=-0.0821, grad=0.3319), Var(v=-0.0111, grad=0.0524), Var(v=0.0064, grad=0.0000), Var(v=0.0032, grad=0.0000), Var(v=-0.1744, grad=0.0000), Var(v=-0.0852, grad=0.0000), Var(v=0.2706, grad=0.0000), Var(v=-0.0491, grad=0.0000), Var(v=-0.0234, grad=0.5250), Var(v=-0.0835, grad=0.1594), Var(v=-0.0663, grad=0.0000), Var(v=0.1334, grad=0.0000), Var(v=0.0081, grad=0.8907), Var(v=-0.0026, grad=0.0129), Var(v=-0.0791, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=0.1167, grad=0.0000), Var(v=-0.1581, grad=0.0000), Var(v=-0.1009, grad=0.0000), Var(v=-0.1836, grad=0.0000), Var(v=-0.0128, grad=-1.3304), Var(v=-0.0247, grad=0.9482), Var(v=-0.0561, grad=0.0000), Var(v=0.1121, grad=0.0000), Var(v=-0.0626, grad=0.3054), Var(v=-0.0781, grad=0.0000), Var(v=0.0012, grad=-0.1773), Var(v=0.1505, grad=-0.8848), Var(v=0.0165, grad=0.0000), Var(v=0.0951, grad=0.0000), Var(v=0.0620, grad=0.0000), Var(v=0.0442, grad=0.0000), Var(v=-0.1939, grad=0.0000), Var(v=-0.0426, grad=0.1904), Var(v=-0.0498, grad=0.2032), Var(v=-0.0578, grad=0.0000), Var(v=-0.1415, grad=0.0000), Var(v=0.0792, grad=-0.9004), Var(v=0.0229, grad=-0.0489), Var(v=0.1153, grad=-0.6021), Var(v=0.0974, grad=-0.6545)], [Var(v=0.0290, grad=0.0000), Var(v=0.1878, grad=0.1153), Var(v=-0.0621, grad=0.0000), Var(v=0.0276, grad=0.2952), Var(v=0.0340, grad=0.0465), Var(v=-0.0643, grad=0.0000), Var(v=0.0410, grad=0.0000), Var(v=-0.0660, grad=0.1177), Var(v=-0.1584, grad=0.0000), Var(v=0.1027, grad=0.0000), Var(v=0.1066, grad=-0.0093), Var(v=0.1469, grad=0.0000), Var(v=-0.1122, grad=-0.1422), Var(v=-0.0054, grad=0.0000), Var(v=0.1254, grad=0.0000), Var(v=0.0631, grad=-0.1047), Var(v=0.1673, grad=-0.0207), Var(v=0.0878, grad=-0.0927), Var(v=0.1521, grad=0.0000), Var(v=-0.0590, grad=-0.3019), Var(v=-0.0030, grad=-0.0484), Var(v=0.0427, grad=-0.1573), Var(v=-0.0029, grad=-0.0023), Var(v=0.0509, grad=-0.0713), Var(v=0.0687, grad=0.0000), Var(v=0.0423, grad=0.1507), Var(v=0.0308, grad=0.0000), Var(v=0.0751, grad=0.0549), Var(v=-0.0261, grad=0.0886), Var(v=0.1456, grad=0.2350), Var(v=-0.0371, grad=0.0000), Var(v=-0.2732, grad=0.0089), Var(v=0.0780, grad=-0.1773), Var(v=0.0803, grad=-0.0539), Var(v=0.1249, grad=0.0007), Var(v=-0.0792, grad=0.0000), Var(v=0.0557, grad=0.1563), Var(v=-0.0120, grad=0.0000), Var(v=-0.0828, grad=0.1515), Var(v=-0.0099, grad=0.0000), Var(v=-0.0559, grad=0.0168), Var(v=0.0133, grad=0.0000), Var(v=0.0104, grad=0.0000), Var(v=0.0865, grad=0.0000), Var(v=0.0370, grad=0.0806), Var(v=0.1116, grad=0.0000), Var(v=0.0766, grad=0.0000), Var(v=0.0337, grad=0.0000), Var(v=-0.1785, grad=0.0000), Var(v=-0.0878, grad=0.1156)], [Var(v=0.0823, grad=-0.1792), Var(v=-0.0541, grad=0.0000), Var(v=-0.0978, grad=0.2133), Var(v=0.0267, grad=-0.8240), Var(v=0.0059, grad=0.0000), Var(v=-0.1164, grad=0.0000), Var(v=-0.1076, grad=0.1358), Var(v=-0.1305, grad=-0.3286), Var(v=0.0157, grad=0.0000), Var(v=-0.0160, grad=0.1637), Var(v=-0.0295, grad=0.0258), Var(v=-0.0993, grad=0.0000), Var(v=-0.2383, grad=0.0000), Var(v=0.1590, grad=0.0000), Var(v=0.2111, grad=0.0000), Var(v=-0.1469, grad=0.0000), Var(v=-0.0489, grad=0.0000), Var(v=0.1185, grad=0.2589), Var(v=0.0051, grad=0.0786), Var(v=-0.0924, grad=0.0000), Var(v=-0.1577, grad=0.0000), Var(v=-0.0977, grad=0.4391), Var(v=0.1530, grad=0.0063), Var(v=0.1494, grad=0.0000), Var(v=-0.2201, grad=0.0000), Var(v=0.2262, grad=0.0000), Var(v=0.0505, grad=0.0000), Var(v=0.1703, grad=0.0000), Var(v=-0.1734, grad=0.0000), Var(v=0.1251, grad=-0.6559), Var(v=-0.0070, grad=0.4675), Var(v=-0.0267, grad=0.0000), Var(v=0.0440, grad=0.0000), Var(v=-0.0084, grad=0.1506), Var(v=0.0900, grad=0.0000), Var(v=-0.0135, grad=-0.0874), Var(v=0.2490, grad=-0.4362), Var(v=-0.0450, grad=0.0000), Var(v=0.0546, grad=0.0000), Var(v=0.0519, grad=0.0000), Var(v=-0.0788, grad=0.0000), Var(v=0.1319, grad=0.0000), Var(v=0.0170, grad=0.0939), Var(v=0.1315, grad=0.1002), Var(v=-0.0050, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1521, grad=-0.4439), Var(v=0.1202, grad=-0.0241), Var(v=-0.0596, grad=-0.2969), Var(v=-0.0274, grad=-0.3227)], [Var(v=-0.0058, grad=0.0000), Var(v=-0.0246, grad=0.1456), Var(v=-0.0979, grad=0.0000), Var(v=0.1720, grad=0.3729), Var(v=0.0508, grad=0.0587), Var(v=0.0093, grad=0.0000), Var(v=-0.0094, grad=0.0000), Var(v=0.0558, grad=0.1487), Var(v=-0.0742, grad=0.0000), Var(v=0.0111, grad=0.0000), Var(v=0.0707, grad=-0.0117), Var(v=0.0641, grad=0.0000), Var(v=0.0081, grad=-0.1797), Var(v=-0.0334, grad=0.0000), Var(v=-0.0411, grad=0.0000), Var(v=0.1537, grad=-0.1322), Var(v=0.1717, grad=-0.0262), Var(v=0.0500, grad=-0.1172), Var(v=-0.0394, grad=0.0000), Var(v=0.1220, grad=-0.3814), Var(v=-0.0248, grad=-0.0611), Var(v=0.0216, grad=-0.1987), Var(v=0.1799, grad=-0.0029), Var(v=-0.0650, grad=-0.0900), Var(v=0.0865, grad=0.0000), Var(v=-0.0022, grad=0.1904), Var(v=0.0953, grad=0.0000), Var(v=-0.0003, grad=0.0693), Var(v=0.0643, grad=0.1119), Var(v=-0.0921, grad=0.2969), Var(v=-0.1024, grad=0.0000), Var(v=-0.0937, grad=0.0113), Var(v=-0.1186, grad=-0.2240), Var(v=0.0959, grad=-0.0681), Var(v=0.0218, grad=0.0009), Var(v=-0.0063, grad=0.0000), Var(v=-0.1614, grad=0.1974), Var(v=-0.3484, grad=0.0000), Var(v=0.0716, grad=0.1913), Var(v=0.0073, grad=0.0000), Var(v=-0.0975, grad=0.0212), Var(v=-0.2139, grad=0.0000), Var(v=-0.1293, grad=0.0000), Var(v=0.0286, grad=0.0000), Var(v=0.1957, grad=0.1018), Var(v=-0.1361, grad=0.0000), Var(v=0.0635, grad=0.0000), Var(v=0.0862, grad=0.0000), Var(v=0.0302, grad=0.0000), Var(v=0.0850, grad=0.1460)], [Var(v=-0.1533, grad=-0.0572), Var(v=-0.0536, grad=0.0000), Var(v=0.0482, grad=0.0681), Var(v=-0.0567, grad=-0.2631), Var(v=-0.0403, grad=0.0000), Var(v=-0.1175, grad=0.0000), Var(v=-0.1346, grad=0.0434), Var(v=0.0942, grad=-0.1049), Var(v=0.1426, grad=0.0000), Var(v=0.0509, grad=0.0523), Var(v=-0.1052, grad=0.0083), Var(v=0.0234, grad=0.0000), Var(v=0.0223, grad=0.0000), Var(v=0.1290, grad=0.0000), Var(v=0.1047, grad=0.0000), Var(v=-0.0152, grad=0.0000), Var(v=-0.1527, grad=0.0000), Var(v=-0.1204, grad=0.0827), Var(v=-0.1294, grad=0.0251), Var(v=-0.0739, grad=0.0000), Var(v=-0.0619, grad=0.0000), Var(v=-0.0038, grad=0.1402), Var(v=0.0260, grad=0.0020), Var(v=0.0766, grad=0.0000), Var(v=0.0633, grad=0.0000), Var(v=0.0898, grad=0.0000), Var(v=0.0056, grad=0.0000), Var(v=0.1841, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=0.0500, grad=-0.2094), Var(v=0.0088, grad=0.1493), Var(v=-0.0227, grad=0.0000), Var(v=-0.1125, grad=0.0000), Var(v=0.0438, grad=0.0481), Var(v=-0.0978, grad=0.0000), Var(v=0.0359, grad=-0.0279), Var(v=-0.1422, grad=-0.1393), Var(v=-0.0107, grad=0.0000), Var(v=0.0673, grad=0.0000), Var(v=-0.1039, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=0.0755, grad=0.0000), Var(v=0.1636, grad=0.0300), Var(v=0.1260, grad=0.0320), Var(v=-0.1480, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=-0.0464, grad=-0.1417), Var(v=-0.0536, grad=-0.0077), Var(v=-0.0252, grad=-0.0948), Var(v=0.0364, grad=-0.1030)], [Var(v=0.1489, grad=-0.1766), Var(v=0.1051, grad=0.0000), Var(v=-0.1025, grad=0.2102), Var(v=0.0521, grad=-0.8121), Var(v=-0.0034, grad=0.0000), Var(v=-0.2291, grad=0.0000), Var(v=-0.0006, grad=0.1338), Var(v=-0.0035, grad=-0.3239), Var(v=-0.0572, grad=0.0000), Var(v=-0.0710, grad=0.1613), Var(v=-0.0585, grad=0.0255), Var(v=-0.2074, grad=0.0000), Var(v=-0.0407, grad=0.0000), Var(v=-0.1832, grad=0.0000), Var(v=-0.0378, grad=0.0000), Var(v=-0.0342, grad=0.0000), Var(v=-0.0596, grad=0.0000), Var(v=0.0432, grad=0.2551), Var(v=0.0781, grad=0.0775), Var(v=-0.0195, grad=0.0000), Var(v=-0.2161, grad=0.0000), Var(v=-0.0064, grad=0.4328), Var(v=0.0596, grad=0.0062), Var(v=-0.1000, grad=0.0000), Var(v=0.1419, grad=0.0000), Var(v=0.0474, grad=0.0000), Var(v=-0.0368, grad=0.0000), Var(v=-0.0547, grad=0.0000), Var(v=-0.0211, grad=0.0000), Var(v=0.0374, grad=-0.6465), Var(v=-0.0089, grad=0.4608), Var(v=-0.0830, grad=0.0000), Var(v=0.0054, grad=0.0000), Var(v=0.0036, grad=0.1484), Var(v=-0.0677, grad=0.0000), Var(v=0.0174, grad=-0.0862), Var(v=-0.1212, grad=-0.4300), Var(v=-0.2066, grad=0.0000), Var(v=-0.1513, grad=0.0000), Var(v=0.0155, grad=0.0000), Var(v=-0.1155, grad=0.0000), Var(v=0.0592, grad=0.0000), Var(v=0.0319, grad=0.0925), Var(v=-0.0139, grad=0.0988), Var(v=-0.0470, grad=0.0000), Var(v=0.1156, grad=0.0000), Var(v=-0.1869, grad=-0.4375), Var(v=-0.0717, grad=-0.0238), Var(v=-0.1247, grad=-0.2926), Var(v=0.0191, grad=-0.3181)], [Var(v=0.0571, grad=-1.0131), Var(v=-0.0124, grad=0.0000), Var(v=0.0859, grad=1.2062), Var(v=0.0659, grad=-4.6592), Var(v=0.0741, grad=0.0000), Var(v=0.0470, grad=0.0000), Var(v=0.0246, grad=0.7677), Var(v=0.1102, grad=-1.8582), Var(v=-0.1036, grad=0.0000), Var(v=0.2137, grad=0.9254), Var(v=0.0749, grad=0.1461), Var(v=-0.0794, grad=0.0000), Var(v=-0.0337, grad=0.0000), Var(v=-0.0645, grad=0.0000), Var(v=-0.0461, grad=0.0000), Var(v=-0.0137, grad=0.0000), Var(v=-0.0253, grad=0.0000), Var(v=0.1289, grad=1.4638), Var(v=-0.0197, grad=0.4444), Var(v=-0.0317, grad=0.0000), Var(v=-0.0496, grad=0.0000), Var(v=-0.0943, grad=2.4831), Var(v=-0.0084, grad=0.0359), Var(v=0.0987, grad=0.0000), Var(v=-0.0086, grad=0.0000), Var(v=-0.1868, grad=0.0000), Var(v=-0.1444, grad=0.0000), Var(v=-0.2083, grad=0.0000), Var(v=-0.0356, grad=0.0000), Var(v=0.1229, grad=-3.7089), Var(v=0.0900, grad=2.6434), Var(v=-0.0216, grad=0.0000), Var(v=-0.0268, grad=0.0000), Var(v=0.1323, grad=0.8514), Var(v=-0.1733, grad=0.0000), Var(v=0.0759, grad=-0.4944), Var(v=0.0123, grad=-2.4668), Var(v=-0.0662, grad=0.0000), Var(v=-0.1196, grad=0.0000), Var(v=-0.0838, grad=0.0000), Var(v=-0.1491, grad=0.0000), Var(v=-0.0845, grad=0.0000), Var(v=0.1153, grad=0.5308), Var(v=0.0939, grad=0.5666), Var(v=-0.1012, grad=0.0000), Var(v=-0.0443, grad=0.0000), Var(v=0.1371, grad=-2.5101), Var(v=0.0017, grad=-0.1364), Var(v=0.1620, grad=-1.6787), Var(v=0.0764, grad=-1.8247)], [Var(v=-0.0225, grad=-0.3392), Var(v=0.1257, grad=0.0000), Var(v=0.0734, grad=0.4038), Var(v=-0.0417, grad=-1.5599), Var(v=-0.1021, grad=0.0000), Var(v=-0.0466, grad=0.0000), Var(v=0.1164, grad=0.2570), Var(v=-0.0358, grad=-0.6221), Var(v=0.0900, grad=0.0000), Var(v=0.0530, grad=0.3098), Var(v=0.1704, grad=0.0489), Var(v=-0.0953, grad=0.0000), Var(v=-0.0705, grad=0.0000), Var(v=0.0412, grad=0.0000), Var(v=0.0193, grad=0.0000), Var(v=-0.1945, grad=0.0000), Var(v=-0.0476, grad=0.0000), Var(v=0.1033, grad=0.4901), Var(v=0.1506, grad=0.1488), Var(v=-0.0862, grad=0.0000), Var(v=0.2004, grad=0.0000), Var(v=0.1000, grad=0.8314), Var(v=0.1198, grad=0.0120), Var(v=-0.1280, grad=0.0000), Var(v=-0.0394, grad=0.0000), Var(v=-0.0353, grad=0.0000), Var(v=0.0256, grad=0.0000), Var(v=0.1017, grad=0.0000), Var(v=0.0163, grad=0.0000), Var(v=-0.1221, grad=-1.2418), Var(v=0.0407, grad=0.8850), Var(v=0.0182, grad=0.0000), Var(v=-0.1387, grad=0.0000), Var(v=-0.0452, grad=0.2851), Var(v=0.0962, grad=0.0000), Var(v=-0.0290, grad=-0.1655), Var(v=-0.1538, grad=-0.8259), Var(v=-0.0200, grad=0.0000), Var(v=-0.0232, grad=0.0000), Var(v=-0.0005, grad=0.0000), Var(v=-0.1554, grad=0.0000), Var(v=0.1371, grad=0.0000), Var(v=0.0982, grad=0.1777), Var(v=0.0185, grad=0.1897), Var(v=0.0323, grad=0.0000), Var(v=-0.0431, grad=0.0000), Var(v=-0.1580, grad=-0.8404), Var(v=-0.0004, grad=-0.0457), Var(v=-0.0863, grad=-0.5620), Var(v=-0.0796, grad=-0.6109)], [Var(v=-0.1602, grad=0.0000), Var(v=0.0178, grad=1.6840), Var(v=-0.0804, grad=0.0000), Var(v=0.1046, grad=4.3132), Var(v=0.1008, grad=0.6793), Var(v=-0.0453, grad=0.0000), Var(v=-0.1003, grad=0.0000), Var(v=0.0616, grad=1.7202), Var(v=-0.1378, grad=0.0000), Var(v=-0.0973, grad=0.0000), Var(v=0.1355, grad=-0.1353), Var(v=0.0059, grad=0.0000), Var(v=0.1464, grad=-2.0783), Var(v=-0.1167, grad=0.0000), Var(v=-0.0093, grad=0.0000), Var(v=0.0217, grad=-1.5293), Var(v=0.0255, grad=-0.3026), Var(v=0.1529, grad=-1.3550), Var(v=-0.0975, grad=0.0000), Var(v=0.0796, grad=-4.4108), Var(v=0.0141, grad=-0.7068), Var(v=0.0327, grad=-2.2987), Var(v=0.1216, grad=-0.0332), Var(v=0.0232, grad=-1.0411), Var(v=-0.0544, grad=0.0000), Var(v=0.1701, grad=2.2022), Var(v=-0.0900, grad=0.0000), Var(v=0.0876, grad=0.8016), Var(v=0.1231, grad=1.2945), Var(v=0.0823, grad=3.4335), Var(v=-0.0037, grad=0.0000), Var(v=-0.0026, grad=0.1304), Var(v=0.0960, grad=-2.5910), Var(v=0.1097, grad=-0.7882), Var(v=0.0854, grad=0.0106), Var(v=-0.0215, grad=0.0000), Var(v=0.1268, grad=2.2836), Var(v=-0.0217, grad=0.0000), Var(v=0.0500, grad=2.2131), Var(v=-0.0989, grad=0.0000), Var(v=0.0884, grad=0.2453), Var(v=-0.1231, grad=0.0000), Var(v=-0.0682, grad=0.0000), Var(v=-0.0814, grad=0.0000), Var(v=0.2068, grad=1.1777), Var(v=-0.0701, grad=0.0000), Var(v=-0.0328, grad=0.0000), Var(v=-0.0358, grad=0.0000), Var(v=-0.0930, grad=0.0000), Var(v=0.1773, grad=1.6892)], [Var(v=0.1631, grad=-0.6253), Var(v=-0.0081, grad=0.0000), Var(v=0.0400, grad=0.7445), Var(v=0.0053, grad=-2.8760), Var(v=0.0544, grad=0.0000), Var(v=0.0847, grad=0.0000), Var(v=0.0480, grad=0.4739), Var(v=0.0426, grad=-1.1470), Var(v=0.1040, grad=0.0000), Var(v=0.0624, grad=0.5712), Var(v=-0.0309, grad=0.0902), Var(v=0.0324, grad=0.0000), Var(v=-0.1673, grad=0.0000), Var(v=0.1526, grad=0.0000), Var(v=0.0984, grad=0.0000), Var(v=0.1112, grad=0.0000), Var(v=-0.0370, grad=0.0000), Var(v=-0.1422, grad=0.9035), Var(v=-0.0439, grad=0.2743), Var(v=0.1197, grad=0.0000), Var(v=-0.0249, grad=0.0000), Var(v=0.0122, grad=1.5328), Var(v=0.0041, grad=0.0221), Var(v=-0.1564, grad=0.0000), Var(v=-0.2191, grad=0.0000), Var(v=-0.1411, grad=0.0000), Var(v=-0.0745, grad=0.0000), Var(v=-0.0121, grad=0.0000), Var(v=0.1296, grad=0.0000), Var(v=-0.1744, grad=-2.2894), Var(v=-0.1015, grad=1.6317), Var(v=0.1353, grad=0.0000), Var(v=-0.3063, grad=0.0000), Var(v=0.0662, grad=0.5255), Var(v=-0.0532, grad=0.0000), Var(v=0.1485, grad=-0.3052), Var(v=0.0920, grad=-1.5227), Var(v=-0.2133, grad=0.0000), Var(v=0.0514, grad=0.0000), Var(v=-0.0289, grad=0.0000), Var(v=-0.2248, grad=0.0000), Var(v=0.0461, grad=0.0000), Var(v=-0.0269, grad=0.3276), Var(v=0.0560, grad=0.3497), Var(v=-0.0450, grad=0.0000), Var(v=-0.0624, grad=0.0000), Var(v=0.0694, grad=-1.5494), Var(v=0.0415, grad=-0.0842), Var(v=0.0288, grad=-1.0362), Var(v=-0.0082, grad=-1.1263)], [Var(v=0.0534, grad=-0.5022), Var(v=0.0917, grad=0.0000), Var(v=0.0536, grad=0.5980), Var(v=0.1159, grad=-2.3098), Var(v=-0.0154, grad=0.0000), Var(v=-0.1351, grad=0.0000), Var(v=0.1547, grad=0.3806), Var(v=0.1562, grad=-0.9212), Var(v=-0.1658, grad=0.0000), Var(v=-0.2151, grad=0.4588), Var(v=-0.0147, grad=0.0725), Var(v=-0.1135, grad=0.0000), Var(v=-0.1935, grad=0.0000), Var(v=0.0483, grad=0.0000), Var(v=-0.0971, grad=0.0000), Var(v=-0.1751, grad=0.0000), Var(v=-0.0424, grad=0.0000), Var(v=-0.0355, grad=0.7257), Var(v=0.2113, grad=0.2203), Var(v=-0.1110, grad=0.0000), Var(v=0.0075, grad=0.0000), Var(v=0.0835, grad=1.2310), Var(v=0.1353, grad=0.0178), Var(v=-0.0624, grad=0.0000), Var(v=0.1825, grad=0.0000), Var(v=-0.0304, grad=0.0000), Var(v=0.0346, grad=0.0000), Var(v=-0.1420, grad=0.0000), Var(v=-0.0140, grad=0.0000), Var(v=-0.0060, grad=-1.8387), Var(v=-0.0028, grad=1.3105), Var(v=-0.1047, grad=0.0000), Var(v=0.0906, grad=0.0000), Var(v=0.0405, grad=0.4221), Var(v=-0.0606, grad=0.0000), Var(v=-0.0755, grad=-0.2451), Var(v=0.0897, grad=-1.2229), Var(v=-0.0258, grad=0.0000), Var(v=-0.0365, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=-0.2374, grad=0.0000), Var(v=0.0131, grad=0.0000), Var(v=-0.0241, grad=0.2631), Var(v=-0.1442, grad=0.2809), Var(v=-0.1482, grad=0.0000), Var(v=0.0182, grad=0.0000), Var(v=-0.1244, grad=-1.2444), Var(v=0.1010, grad=-0.0676), Var(v=0.0848, grad=-0.8322), Var(v=0.1170, grad=-0.9046)], [Var(v=-0.1426, grad=0.0000), Var(v=-0.0885, grad=0.4727), Var(v=0.1264, grad=0.0000), Var(v=-0.0148, grad=1.2107), Var(v=0.1416, grad=0.1907), Var(v=0.0151, grad=0.0000), Var(v=0.0394, grad=0.0000), Var(v=0.0948, grad=0.4828), Var(v=0.0323, grad=0.0000), Var(v=0.1841, grad=0.0000), Var(v=0.2543, grad=-0.0380), Var(v=-0.2196, grad=0.0000), Var(v=0.1008, grad=-0.5834), Var(v=-0.2187, grad=0.0000), Var(v=-0.0582, grad=0.0000), Var(v=0.2129, grad=-0.4293), Var(v=0.0272, grad=-0.0849), Var(v=-0.1375, grad=-0.3804), Var(v=0.0508, grad=0.0000), Var(v=0.0620, grad=-1.2381), Var(v=0.1014, grad=-0.1984), Var(v=0.0875, grad=-0.6452), Var(v=0.1117, grad=-0.0093), Var(v=-0.0381, grad=-0.2922), Var(v=0.0066, grad=0.0000), Var(v=0.0828, grad=0.6181), Var(v=-0.1084, grad=0.0000), Var(v=-0.1287, grad=0.2250), Var(v=-0.1950, grad=0.3634), Var(v=-0.2756, grad=0.9638), Var(v=-0.0485, grad=0.0000), Var(v=0.2992, grad=0.0366), Var(v=0.0186, grad=-0.7273), Var(v=0.0168, grad=-0.2212), Var(v=0.0646, grad=0.0030), Var(v=-0.0676, grad=0.0000), Var(v=0.0239, grad=0.6410), Var(v=0.0146, grad=0.0000), Var(v=0.0586, grad=0.6212), Var(v=-0.0307, grad=0.0000), Var(v=0.0727, grad=0.0688), Var(v=0.0673, grad=0.0000), Var(v=0.0497, grad=0.0000), Var(v=-0.0441, grad=0.0000), Var(v=0.0271, grad=0.3306), Var(v=0.0905, grad=0.0000), Var(v=0.0167, grad=0.0000), Var(v=-0.1544, grad=0.0000), Var(v=0.0028, grad=0.0000), Var(v=-0.0899, grad=0.4741)], [Var(v=-0.0772, grad=-0.1897), Var(v=-0.1132, grad=0.0000), Var(v=0.0517, grad=0.2259), Var(v=0.0788, grad=-0.8724), Var(v=-0.0670, grad=0.0000), Var(v=0.0840, grad=0.0000), Var(v=-0.0300, grad=0.1438), Var(v=-0.0979, grad=-0.3479), Var(v=-0.1870, grad=0.0000), Var(v=0.0429, grad=0.1733), Var(v=-0.0792, grad=0.0274), Var(v=0.1164, grad=0.0000), Var(v=0.0526, grad=0.0000), Var(v=-0.0680, grad=0.0000), Var(v=-0.0784, grad=0.0000), Var(v=-0.0779, grad=0.0000), Var(v=0.1171, grad=0.0000), Var(v=-0.1185, grad=0.2741), Var(v=0.0820, grad=0.0832), Var(v=-0.1726, grad=0.0000), Var(v=-0.1346, grad=0.0000), Var(v=-0.0147, grad=0.4650), Var(v=0.0619, grad=0.0067), Var(v=0.1117, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=-0.0416, grad=0.0000), Var(v=-0.0690, grad=0.0000), Var(v=0.0169, grad=0.0000), Var(v=-0.0663, grad=0.0000), Var(v=0.1333, grad=-0.6945), Var(v=-0.1035, grad=0.4950), Var(v=0.0308, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=0.0609, grad=0.1594), Var(v=0.1364, grad=0.0000), Var(v=-0.1620, grad=-0.0926), Var(v=-0.0279, grad=-0.4619), Var(v=0.0045, grad=0.0000), Var(v=0.0505, grad=0.0000), Var(v=-0.0200, grad=0.0000), Var(v=-0.0070, grad=0.0000), Var(v=-0.1070, grad=0.0000), Var(v=0.0653, grad=0.0994), Var(v=-0.0269, grad=0.1061), Var(v=0.0343, grad=0.0000), Var(v=0.0972, grad=0.0000), Var(v=0.0786, grad=-0.4700), Var(v=0.0462, grad=-0.0256), Var(v=-0.0758, grad=-0.3143), Var(v=-0.1059, grad=-0.3417)]] Biases: [Var(v=0.0000, grad=-4.8775), Var(v=0.0000, grad=8.9951), Var(v=0.0000, grad=5.8072), Var(v=0.0000, grad=0.6072), Var(v=0.0000, grad=3.6283), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=3.6963), Var(v=0.0000, grad=0.2422), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=4.4555), Var(v=0.0000, grad=-0.0190), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-11.1012), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-8.1691), Var(v=0.0000, grad=-1.6163), Var(v=0.0000, grad=-0.1908), Var(v=0.0000, grad=2.1394), Var(v=0.0000, grad=-23.5608), Var(v=0.0000, grad=-3.7756), Var(v=0.0000, grad=-0.3236), Var(v=0.0000, grad=-0.0047), Var(v=0.0000, grad=-5.5612), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=11.7632), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=4.2816), Var(v=0.0000, grad=6.9148), Var(v=0.0000, grad=0.4834), Var(v=0.0000, grad=12.7266), Var(v=0.0000, grad=0.6963), Var(v=0.0000, grad=-13.8402), Var(v=0.0000, grad=-0.1110), Var(v=0.0000, grad=0.0569), Var(v=0.0000, grad=-2.3802), Var(v=0.0000, grad=0.3215), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=11.8214), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=1.3102), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=2.5555), Var(v=0.0000, grad=2.7277), Var(v=0.0000, grad=6.2907), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-12.0851), Var(v=0.0000, grad=-0.6569), Var(v=0.0000, grad=-8.0820), Var(v=0.0000, grad=0.2378)]\n",
            "Layer 2 \n",
            " Weights: [[Var(v=-0.0570, grad=2.6895)], [Var(v=-0.1024, grad=-0.0612)], [Var(v=0.0679, grad=2.0915)], [Var(v=-0.2623, grad=0.5607)], [Var(v=-0.0413, grad=-2.4227)], [Var(v=-0.1486, grad=0.0000)], [Var(v=0.0432, grad=3.2507)], [Var(v=-0.1046, grad=2.8451)], [Var(v=-0.0833, grad=0.0000)], [Var(v=0.0521, grad=1.9005)], [Var(v=0.0082, grad=-1.8894)], [Var(v=0.0255, grad=0.0000)], [Var(v=0.1264, grad=-2.7591)], [Var(v=-0.2298, grad=0.0000)], [Var(v=-0.1519, grad=0.0000)], [Var(v=0.0930, grad=-1.6287)], [Var(v=0.0184, grad=-0.9769)], [Var(v=0.0824, grad=0.1473)], [Var(v=0.0250, grad=3.6075)], [Var(v=0.2682, grad=-1.7029)], [Var(v=0.0430, grad=-0.6615)], [Var(v=0.1398, grad=-0.9493)], [Var(v=0.0020, grad=0.8877)], [Var(v=0.0633, grad=-0.1709)], [Var(v=-0.0502, grad=0.0000)], [Var(v=-0.1339, grad=-3.2242)], [Var(v=0.0748, grad=0.0000)], [Var(v=-0.0487, grad=-0.9311)], [Var(v=-0.0787, grad=-1.1864)], [Var(v=-0.2088, grad=0.9448)], [Var(v=0.1488, grad=0.3536)], [Var(v=-0.0079, grad=-0.8981)], [Var(v=0.1575, grad=-1.5844)], [Var(v=0.0479, grad=-0.8917)], [Var(v=-0.0006, grad=-1.8743)], [Var(v=-0.0278, grad=0.7007)], [Var(v=-0.1388, grad=0.6573)], [Var(v=-0.0316, grad=0.0000)], [Var(v=-0.1346, grad=-1.1015)], [Var(v=-0.0368, grad=0.0000)], [Var(v=-0.0149, grad=-1.5878)], [Var(v=0.1221, grad=0.0000)], [Var(v=0.0299, grad=2.0252)], [Var(v=0.0319, grad=4.4336)], [Var(v=-0.0716, grad=-3.8468)], [Var(v=0.1181, grad=0.0000)], [Var(v=-0.1413, grad=2.1066)], [Var(v=-0.0077, grad=0.0305)], [Var(v=-0.0945, grad=3.3618)], [Var(v=-0.1027, grad=0.1399)]] Biases: [Var(v=0.0000, grad=-2.3154)]\n",
            "\n",
            "Network after update:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=-0.0166, grad=1.3311), Var(v=-0.1257, grad=-0.2807), Var(v=-0.1512, grad=8.3540), Var(v=-0.0235, grad=3.4471), Var(v=-0.1133, grad=7.9912), Var(v=0.0100, grad=0.3893), Var(v=0.0134, grad=-2.4088), Var(v=-0.0174, grad=-1.5518), Var(v=-0.2474, grad=5.8693), Var(v=0.0783, grad=-14.1517), Var(v=0.0901, grad=7.0739), Var(v=-0.1371, grad=2.0652), Var(v=-0.1431, grad=4.9511), Var(v=0.1642, grad=-11.9045), Var(v=-0.0693, grad=3.4011)]] Biases: [Var(v=0.0121, grad=-1.2095), Var(v=-0.0026, grad=0.2551), Var(v=0.0759, grad=-7.5907), Var(v=-0.0296, grad=2.9607), Var(v=0.0726, grad=-7.2611), Var(v=-0.0033, grad=0.3344), Var(v=-0.0219, grad=2.1887), Var(v=-0.0141, grad=1.4101), Var(v=0.0533, grad=-5.3330), Var(v=-0.1286, grad=12.8587), Var(v=-0.0608, grad=6.0757), Var(v=0.0188, grad=-1.8765), Var(v=0.0450, grad=-4.4988), Var(v=0.1022, grad=-10.2247), Var(v=0.0309, grad=-3.0903)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.0341, grad=-0.0176), Var(v=0.2596, grad=0.0000), Var(v=0.0326, grad=0.0209), Var(v=0.0805, grad=-0.0808), Var(v=0.1167, grad=0.0000), Var(v=0.1777, grad=0.0000), Var(v=-0.1693, grad=0.0133), Var(v=-0.0881, grad=-0.0322), Var(v=-0.0355, grad=0.0000), Var(v=-0.1079, grad=0.0160), Var(v=0.1215, grad=0.0025), Var(v=-0.1300, grad=0.0000), Var(v=0.0500, grad=0.0000), Var(v=-0.0657, grad=0.0000), Var(v=-0.1067, grad=0.0000), Var(v=0.0295, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.2482, grad=0.0254), Var(v=0.1019, grad=0.0077), Var(v=0.0111, grad=0.0000), Var(v=-0.1020, grad=0.0000), Var(v=-0.0077, grad=0.0430), Var(v=-0.0215, grad=0.0006), Var(v=-0.0346, grad=0.0000), Var(v=-0.0588, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=-0.0412, grad=0.0000), Var(v=-0.0345, grad=0.0000), Var(v=-0.0248, grad=0.0000), Var(v=-0.0757, grad=-0.0643), Var(v=-0.0290, grad=0.0458), Var(v=-0.1563, grad=0.0000), Var(v=0.0072, grad=0.0000), Var(v=-0.1274, grad=0.0148), Var(v=0.0047, grad=0.0000), Var(v=0.3267, grad=-0.0086), Var(v=-0.0204, grad=-0.0428), Var(v=0.1360, grad=0.0000), Var(v=-0.1031, grad=0.0000), Var(v=-0.0361, grad=0.0000), Var(v=-0.1102, grad=0.0000), Var(v=-0.0052, grad=0.0000), Var(v=-0.0488, grad=0.0092), Var(v=0.0106, grad=0.0098), Var(v=0.0507, grad=0.0000), Var(v=-0.1748, grad=0.0000), Var(v=-0.0909, grad=-0.0435), Var(v=0.0416, grad=-0.0024), Var(v=0.0623, grad=-0.0291), Var(v=-0.1864, grad=-0.0316)], [Var(v=-0.0725, grad=-0.6899), Var(v=-0.0818, grad=0.0000), Var(v=-0.0426, grad=0.8213), Var(v=0.0298, grad=-3.1726), Var(v=-0.2672, grad=0.0000), Var(v=-0.0861, grad=0.0000), Var(v=0.0647, grad=0.5228), Var(v=0.0347, grad=-1.2653), Var(v=-0.1109, grad=0.0000), Var(v=-0.0367, grad=0.6302), Var(v=0.0432, grad=0.0995), Var(v=0.0006, grad=0.0000), Var(v=-0.0567, grad=0.0000), Var(v=-0.0825, grad=0.0000), Var(v=-0.1025, grad=0.0000), Var(v=0.0132, grad=0.0000), Var(v=-0.0494, grad=0.0000), Var(v=0.1058, grad=0.9967), Var(v=0.1442, grad=0.3026), Var(v=-0.0520, grad=0.0000), Var(v=-0.1259, grad=0.0000), Var(v=0.0338, grad=1.6909), Var(v=0.0808, grad=0.0244), Var(v=0.0061, grad=0.0000), Var(v=-0.0026, grad=0.0000), Var(v=-0.0059, grad=0.0000), Var(v=0.1792, grad=0.0000), Var(v=0.0162, grad=0.0000), Var(v=-0.0467, grad=0.0000), Var(v=0.0806, grad=-2.5256), Var(v=-0.0013, grad=1.8000), Var(v=-0.1821, grad=0.0000), Var(v=-0.0426, grad=0.0000), Var(v=-0.1496, grad=0.5798), Var(v=-0.1210, grad=0.0000), Var(v=-0.0839, grad=-0.3366), Var(v=0.0547, grad=-1.6797), Var(v=0.0778, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=0.0102, grad=0.0000), Var(v=0.1392, grad=0.0000), Var(v=-0.0149, grad=0.0000), Var(v=-0.0325, grad=0.3614), Var(v=0.2621, grad=0.3858), Var(v=-0.0205, grad=0.0000), Var(v=0.0265, grad=0.0000), Var(v=0.0464, grad=-1.7093), Var(v=-0.1443, grad=-0.0929), Var(v=0.0139, grad=-1.1431), Var(v=0.0635, grad=-1.2425)], [Var(v=0.0368, grad=-0.3634), Var(v=-0.0234, grad=0.0000), Var(v=0.0001, grad=0.4327), Var(v=0.0222, grad=-1.6712), Var(v=-0.0649, grad=0.0000), Var(v=-0.0680, grad=0.0000), Var(v=-0.0007, grad=0.2754), Var(v=0.1833, grad=-0.6665), Var(v=0.0335, grad=0.0000), Var(v=-0.0854, grad=0.3319), Var(v=-0.0116, grad=0.0524), Var(v=0.0064, grad=0.0000), Var(v=0.0032, grad=0.0000), Var(v=-0.1744, grad=0.0000), Var(v=-0.0852, grad=0.0000), Var(v=0.2706, grad=0.0000), Var(v=-0.0491, grad=0.0000), Var(v=-0.0286, grad=0.5250), Var(v=-0.0851, grad=0.1594), Var(v=-0.0663, grad=0.0000), Var(v=0.1334, grad=0.0000), Var(v=-0.0008, grad=0.8907), Var(v=-0.0027, grad=0.0129), Var(v=-0.0791, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=0.1167, grad=0.0000), Var(v=-0.1581, grad=0.0000), Var(v=-0.1009, grad=0.0000), Var(v=-0.1836, grad=0.0000), Var(v=0.0005, grad=-1.3304), Var(v=-0.0342, grad=0.9482), Var(v=-0.0561, grad=0.0000), Var(v=0.1121, grad=0.0000), Var(v=-0.0656, grad=0.3054), Var(v=-0.0781, grad=0.0000), Var(v=0.0029, grad=-0.1773), Var(v=0.1593, grad=-0.8848), Var(v=0.0165, grad=0.0000), Var(v=0.0951, grad=0.0000), Var(v=0.0620, grad=0.0000), Var(v=0.0442, grad=0.0000), Var(v=-0.1939, grad=0.0000), Var(v=-0.0445, grad=0.1904), Var(v=-0.0518, grad=0.2032), Var(v=-0.0578, grad=0.0000), Var(v=-0.1415, grad=0.0000), Var(v=0.0882, grad=-0.9004), Var(v=0.0234, grad=-0.0489), Var(v=0.1213, grad=-0.6021), Var(v=0.1039, grad=-0.6545)], [Var(v=0.0290, grad=0.0000), Var(v=0.1867, grad=0.1153), Var(v=-0.0621, grad=0.0000), Var(v=0.0246, grad=0.2952), Var(v=0.0335, grad=0.0465), Var(v=-0.0643, grad=0.0000), Var(v=0.0410, grad=0.0000), Var(v=-0.0672, grad=0.1177), Var(v=-0.1584, grad=0.0000), Var(v=0.1027, grad=0.0000), Var(v=0.1067, grad=-0.0093), Var(v=0.1469, grad=0.0000), Var(v=-0.1108, grad=-0.1422), Var(v=-0.0054, grad=0.0000), Var(v=0.1254, grad=0.0000), Var(v=0.0641, grad=-0.1047), Var(v=0.1676, grad=-0.0207), Var(v=0.0887, grad=-0.0927), Var(v=0.1521, grad=0.0000), Var(v=-0.0560, grad=-0.3019), Var(v=-0.0025, grad=-0.0484), Var(v=0.0443, grad=-0.1573), Var(v=-0.0029, grad=-0.0023), Var(v=0.0516, grad=-0.0713), Var(v=0.0687, grad=0.0000), Var(v=0.0408, grad=0.1507), Var(v=0.0308, grad=0.0000), Var(v=0.0746, grad=0.0549), Var(v=-0.0269, grad=0.0886), Var(v=0.1432, grad=0.2350), Var(v=-0.0371, grad=0.0000), Var(v=-0.2732, grad=0.0089), Var(v=0.0797, grad=-0.1773), Var(v=0.0809, grad=-0.0539), Var(v=0.1249, grad=0.0007), Var(v=-0.0792, grad=0.0000), Var(v=0.0541, grad=0.1563), Var(v=-0.0120, grad=0.0000), Var(v=-0.0843, grad=0.1515), Var(v=-0.0099, grad=0.0000), Var(v=-0.0561, grad=0.0168), Var(v=0.0133, grad=0.0000), Var(v=0.0104, grad=0.0000), Var(v=0.0865, grad=0.0000), Var(v=0.0361, grad=0.0806), Var(v=0.1116, grad=0.0000), Var(v=0.0766, grad=0.0000), Var(v=0.0337, grad=0.0000), Var(v=-0.1785, grad=0.0000), Var(v=-0.0889, grad=0.1156)], [Var(v=0.0841, grad=-0.1792), Var(v=-0.0541, grad=0.0000), Var(v=-0.1000, grad=0.2133), Var(v=0.0349, grad=-0.8240), Var(v=0.0059, grad=0.0000), Var(v=-0.1164, grad=0.0000), Var(v=-0.1089, grad=0.1358), Var(v=-0.1272, grad=-0.3286), Var(v=0.0157, grad=0.0000), Var(v=-0.0177, grad=0.1637), Var(v=-0.0297, grad=0.0258), Var(v=-0.0993, grad=0.0000), Var(v=-0.2383, grad=0.0000), Var(v=0.1590, grad=0.0000), Var(v=0.2111, grad=0.0000), Var(v=-0.1469, grad=0.0000), Var(v=-0.0489, grad=0.0000), Var(v=0.1159, grad=0.2589), Var(v=0.0043, grad=0.0786), Var(v=-0.0924, grad=0.0000), Var(v=-0.1577, grad=0.0000), Var(v=-0.1021, grad=0.4391), Var(v=0.1529, grad=0.0063), Var(v=0.1494, grad=0.0000), Var(v=-0.2201, grad=0.0000), Var(v=0.2262, grad=0.0000), Var(v=0.0505, grad=0.0000), Var(v=0.1703, grad=0.0000), Var(v=-0.1734, grad=0.0000), Var(v=0.1317, grad=-0.6559), Var(v=-0.0117, grad=0.4675), Var(v=-0.0267, grad=0.0000), Var(v=0.0440, grad=0.0000), Var(v=-0.0099, grad=0.1506), Var(v=0.0900, grad=0.0000), Var(v=-0.0126, grad=-0.0874), Var(v=0.2534, grad=-0.4362), Var(v=-0.0450, grad=0.0000), Var(v=0.0546, grad=0.0000), Var(v=0.0519, grad=0.0000), Var(v=-0.0788, grad=0.0000), Var(v=0.1319, grad=0.0000), Var(v=0.0160, grad=0.0939), Var(v=0.1305, grad=0.1002), Var(v=-0.0050, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1565, grad=-0.4439), Var(v=0.1205, grad=-0.0241), Var(v=-0.0566, grad=-0.2969), Var(v=-0.0242, grad=-0.3227)], [Var(v=-0.0058, grad=0.0000), Var(v=-0.0261, grad=0.1456), Var(v=-0.0979, grad=0.0000), Var(v=0.1683, grad=0.3729), Var(v=0.0502, grad=0.0587), Var(v=0.0093, grad=0.0000), Var(v=-0.0094, grad=0.0000), Var(v=0.0543, grad=0.1487), Var(v=-0.0742, grad=0.0000), Var(v=0.0111, grad=0.0000), Var(v=0.0708, grad=-0.0117), Var(v=0.0641, grad=0.0000), Var(v=0.0099, grad=-0.1797), Var(v=-0.0334, grad=0.0000), Var(v=-0.0411, grad=0.0000), Var(v=0.1550, grad=-0.1322), Var(v=0.1720, grad=-0.0262), Var(v=0.0512, grad=-0.1172), Var(v=-0.0394, grad=0.0000), Var(v=0.1258, grad=-0.3814), Var(v=-0.0241, grad=-0.0611), Var(v=0.0236, grad=-0.1987), Var(v=0.1799, grad=-0.0029), Var(v=-0.0641, grad=-0.0900), Var(v=0.0865, grad=0.0000), Var(v=-0.0041, grad=0.1904), Var(v=0.0953, grad=0.0000), Var(v=-0.0010, grad=0.0693), Var(v=0.0632, grad=0.1119), Var(v=-0.0950, grad=0.2969), Var(v=-0.1024, grad=0.0000), Var(v=-0.0938, grad=0.0113), Var(v=-0.1164, grad=-0.2240), Var(v=0.0966, grad=-0.0681), Var(v=0.0218, grad=0.0009), Var(v=-0.0063, grad=0.0000), Var(v=-0.1634, grad=0.1974), Var(v=-0.3484, grad=0.0000), Var(v=0.0696, grad=0.1913), Var(v=0.0073, grad=0.0000), Var(v=-0.0977, grad=0.0212), Var(v=-0.2139, grad=0.0000), Var(v=-0.1293, grad=0.0000), Var(v=0.0286, grad=0.0000), Var(v=0.1947, grad=0.1018), Var(v=-0.1361, grad=0.0000), Var(v=0.0635, grad=0.0000), Var(v=0.0862, grad=0.0000), Var(v=0.0302, grad=0.0000), Var(v=0.0835, grad=0.1460)], [Var(v=-0.1527, grad=-0.0572), Var(v=-0.0536, grad=0.0000), Var(v=0.0475, grad=0.0681), Var(v=-0.0540, grad=-0.2631), Var(v=-0.0403, grad=0.0000), Var(v=-0.1175, grad=0.0000), Var(v=-0.1350, grad=0.0434), Var(v=0.0952, grad=-0.1049), Var(v=0.1426, grad=0.0000), Var(v=0.0504, grad=0.0523), Var(v=-0.1053, grad=0.0083), Var(v=0.0234, grad=0.0000), Var(v=0.0223, grad=0.0000), Var(v=0.1290, grad=0.0000), Var(v=0.1047, grad=0.0000), Var(v=-0.0152, grad=0.0000), Var(v=-0.1527, grad=0.0000), Var(v=-0.1212, grad=0.0827), Var(v=-0.1297, grad=0.0251), Var(v=-0.0739, grad=0.0000), Var(v=-0.0619, grad=0.0000), Var(v=-0.0052, grad=0.1402), Var(v=0.0260, grad=0.0020), Var(v=0.0766, grad=0.0000), Var(v=0.0633, grad=0.0000), Var(v=0.0898, grad=0.0000), Var(v=0.0056, grad=0.0000), Var(v=0.1841, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=0.0521, grad=-0.2094), Var(v=0.0073, grad=0.1493), Var(v=-0.0227, grad=0.0000), Var(v=-0.1125, grad=0.0000), Var(v=0.0433, grad=0.0481), Var(v=-0.0978, grad=0.0000), Var(v=0.0362, grad=-0.0279), Var(v=-0.1408, grad=-0.1393), Var(v=-0.0107, grad=0.0000), Var(v=0.0673, grad=0.0000), Var(v=-0.1039, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=0.0755, grad=0.0000), Var(v=0.1633, grad=0.0300), Var(v=0.1257, grad=0.0320), Var(v=-0.1480, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=-0.0450, grad=-0.1417), Var(v=-0.0535, grad=-0.0077), Var(v=-0.0243, grad=-0.0948), Var(v=0.0374, grad=-0.1030)], [Var(v=0.1507, grad=-0.1766), Var(v=0.1051, grad=0.0000), Var(v=-0.1046, grad=0.2102), Var(v=0.0602, grad=-0.8121), Var(v=-0.0034, grad=0.0000), Var(v=-0.2291, grad=0.0000), Var(v=-0.0020, grad=0.1338), Var(v=-0.0003, grad=-0.3239), Var(v=-0.0572, grad=0.0000), Var(v=-0.0726, grad=0.1613), Var(v=-0.0588, grad=0.0255), Var(v=-0.2074, grad=0.0000), Var(v=-0.0407, grad=0.0000), Var(v=-0.1832, grad=0.0000), Var(v=-0.0378, grad=0.0000), Var(v=-0.0342, grad=0.0000), Var(v=-0.0596, grad=0.0000), Var(v=0.0407, grad=0.2551), Var(v=0.0773, grad=0.0775), Var(v=-0.0195, grad=0.0000), Var(v=-0.2161, grad=0.0000), Var(v=-0.0108, grad=0.4328), Var(v=0.0596, grad=0.0062), Var(v=-0.1000, grad=0.0000), Var(v=0.1419, grad=0.0000), Var(v=0.0474, grad=0.0000), Var(v=-0.0368, grad=0.0000), Var(v=-0.0547, grad=0.0000), Var(v=-0.0211, grad=0.0000), Var(v=0.0439, grad=-0.6465), Var(v=-0.0135, grad=0.4608), Var(v=-0.0830, grad=0.0000), Var(v=0.0054, grad=0.0000), Var(v=0.0021, grad=0.1484), Var(v=-0.0677, grad=0.0000), Var(v=0.0183, grad=-0.0862), Var(v=-0.1169, grad=-0.4300), Var(v=-0.2066, grad=0.0000), Var(v=-0.1513, grad=0.0000), Var(v=0.0155, grad=0.0000), Var(v=-0.1155, grad=0.0000), Var(v=0.0592, grad=0.0000), Var(v=0.0310, grad=0.0925), Var(v=-0.0149, grad=0.0988), Var(v=-0.0470, grad=0.0000), Var(v=0.1156, grad=0.0000), Var(v=-0.1826, grad=-0.4375), Var(v=-0.0715, grad=-0.0238), Var(v=-0.1218, grad=-0.2926), Var(v=0.0223, grad=-0.3181)], [Var(v=0.0672, grad=-1.0131), Var(v=-0.0124, grad=0.0000), Var(v=0.0739, grad=1.2062), Var(v=0.1125, grad=-4.6592), Var(v=0.0741, grad=0.0000), Var(v=0.0470, grad=0.0000), Var(v=0.0169, grad=0.7677), Var(v=0.1287, grad=-1.8582), Var(v=-0.1036, grad=0.0000), Var(v=0.2044, grad=0.9254), Var(v=0.0734, grad=0.1461), Var(v=-0.0794, grad=0.0000), Var(v=-0.0337, grad=0.0000), Var(v=-0.0645, grad=0.0000), Var(v=-0.0461, grad=0.0000), Var(v=-0.0137, grad=0.0000), Var(v=-0.0253, grad=0.0000), Var(v=0.1143, grad=1.4638), Var(v=-0.0241, grad=0.4444), Var(v=-0.0317, grad=0.0000), Var(v=-0.0496, grad=0.0000), Var(v=-0.1192, grad=2.4831), Var(v=-0.0087, grad=0.0359), Var(v=0.0987, grad=0.0000), Var(v=-0.0086, grad=0.0000), Var(v=-0.1868, grad=0.0000), Var(v=-0.1444, grad=0.0000), Var(v=-0.2083, grad=0.0000), Var(v=-0.0356, grad=0.0000), Var(v=0.1600, grad=-3.7089), Var(v=0.0636, grad=2.6434), Var(v=-0.0216, grad=0.0000), Var(v=-0.0268, grad=0.0000), Var(v=0.1238, grad=0.8514), Var(v=-0.1733, grad=0.0000), Var(v=0.0808, grad=-0.4944), Var(v=0.0369, grad=-2.4668), Var(v=-0.0662, grad=0.0000), Var(v=-0.1196, grad=0.0000), Var(v=-0.0838, grad=0.0000), Var(v=-0.1491, grad=0.0000), Var(v=-0.0845, grad=0.0000), Var(v=0.1100, grad=0.5308), Var(v=0.0883, grad=0.5666), Var(v=-0.1012, grad=0.0000), Var(v=-0.0443, grad=0.0000), Var(v=0.1622, grad=-2.5101), Var(v=0.0031, grad=-0.1364), Var(v=0.1788, grad=-1.6787), Var(v=0.0947, grad=-1.8247)], [Var(v=-0.0191, grad=-0.3392), Var(v=0.1257, grad=0.0000), Var(v=0.0693, grad=0.4038), Var(v=-0.0261, grad=-1.5599), Var(v=-0.1021, grad=0.0000), Var(v=-0.0466, grad=0.0000), Var(v=0.1139, grad=0.2570), Var(v=-0.0295, grad=-0.6221), Var(v=0.0900, grad=0.0000), Var(v=0.0499, grad=0.3098), Var(v=0.1699, grad=0.0489), Var(v=-0.0953, grad=0.0000), Var(v=-0.0705, grad=0.0000), Var(v=0.0412, grad=0.0000), Var(v=0.0193, grad=0.0000), Var(v=-0.1945, grad=0.0000), Var(v=-0.0476, grad=0.0000), Var(v=0.0984, grad=0.4901), Var(v=0.1492, grad=0.1488), Var(v=-0.0862, grad=0.0000), Var(v=0.2004, grad=0.0000), Var(v=0.0917, grad=0.8314), Var(v=0.1197, grad=0.0120), Var(v=-0.1280, grad=0.0000), Var(v=-0.0394, grad=0.0000), Var(v=-0.0353, grad=0.0000), Var(v=0.0256, grad=0.0000), Var(v=0.1017, grad=0.0000), Var(v=0.0163, grad=0.0000), Var(v=-0.1096, grad=-1.2418), Var(v=0.0319, grad=0.8850), Var(v=0.0182, grad=0.0000), Var(v=-0.1387, grad=0.0000), Var(v=-0.0481, grad=0.2851), Var(v=0.0962, grad=0.0000), Var(v=-0.0274, grad=-0.1655), Var(v=-0.1455, grad=-0.8259), Var(v=-0.0200, grad=0.0000), Var(v=-0.0232, grad=0.0000), Var(v=-0.0005, grad=0.0000), Var(v=-0.1554, grad=0.0000), Var(v=0.1371, grad=0.0000), Var(v=0.0964, grad=0.1777), Var(v=0.0166, grad=0.1897), Var(v=0.0323, grad=0.0000), Var(v=-0.0431, grad=0.0000), Var(v=-0.1496, grad=-0.8404), Var(v=0.0001, grad=-0.0457), Var(v=-0.0806, grad=-0.5620), Var(v=-0.0735, grad=-0.6109)], [Var(v=-0.1602, grad=0.0000), Var(v=0.0010, grad=1.6840), Var(v=-0.0804, grad=0.0000), Var(v=0.0615, grad=4.3132), Var(v=0.0940, grad=0.6793), Var(v=-0.0453, grad=0.0000), Var(v=-0.1003, grad=0.0000), Var(v=0.0444, grad=1.7202), Var(v=-0.1378, grad=0.0000), Var(v=-0.0973, grad=0.0000), Var(v=0.1368, grad=-0.1353), Var(v=0.0059, grad=0.0000), Var(v=0.1672, grad=-2.0783), Var(v=-0.1167, grad=0.0000), Var(v=-0.0093, grad=0.0000), Var(v=0.0370, grad=-1.5293), Var(v=0.0285, grad=-0.3026), Var(v=0.1665, grad=-1.3550), Var(v=-0.0975, grad=0.0000), Var(v=0.1237, grad=-4.4108), Var(v=0.0212, grad=-0.7068), Var(v=0.0557, grad=-2.2987), Var(v=0.1219, grad=-0.0332), Var(v=0.0336, grad=-1.0411), Var(v=-0.0544, grad=0.0000), Var(v=0.1481, grad=2.2022), Var(v=-0.0900, grad=0.0000), Var(v=0.0796, grad=0.8016), Var(v=0.1102, grad=1.2945), Var(v=0.0480, grad=3.4335), Var(v=-0.0037, grad=0.0000), Var(v=-0.0039, grad=0.1304), Var(v=0.1219, grad=-2.5910), Var(v=0.1176, grad=-0.7882), Var(v=0.0853, grad=0.0106), Var(v=-0.0215, grad=0.0000), Var(v=0.1039, grad=2.2836), Var(v=-0.0217, grad=0.0000), Var(v=0.0279, grad=2.2131), Var(v=-0.0989, grad=0.0000), Var(v=0.0859, grad=0.2453), Var(v=-0.1231, grad=0.0000), Var(v=-0.0682, grad=0.0000), Var(v=-0.0814, grad=0.0000), Var(v=0.1951, grad=1.1777), Var(v=-0.0701, grad=0.0000), Var(v=-0.0328, grad=0.0000), Var(v=-0.0358, grad=0.0000), Var(v=-0.0930, grad=0.0000), Var(v=0.1605, grad=1.6892)], [Var(v=0.1694, grad=-0.6253), Var(v=-0.0081, grad=0.0000), Var(v=0.0325, grad=0.7445), Var(v=0.0340, grad=-2.8760), Var(v=0.0544, grad=0.0000), Var(v=0.0847, grad=0.0000), Var(v=0.0433, grad=0.4739), Var(v=0.0541, grad=-1.1470), Var(v=0.1040, grad=0.0000), Var(v=0.0567, grad=0.5712), Var(v=-0.0318, grad=0.0902), Var(v=0.0324, grad=0.0000), Var(v=-0.1673, grad=0.0000), Var(v=0.1526, grad=0.0000), Var(v=0.0984, grad=0.0000), Var(v=0.1112, grad=0.0000), Var(v=-0.0370, grad=0.0000), Var(v=-0.1513, grad=0.9035), Var(v=-0.0467, grad=0.2743), Var(v=0.1197, grad=0.0000), Var(v=-0.0249, grad=0.0000), Var(v=-0.0031, grad=1.5328), Var(v=0.0039, grad=0.0221), Var(v=-0.1564, grad=0.0000), Var(v=-0.2191, grad=0.0000), Var(v=-0.1411, grad=0.0000), Var(v=-0.0745, grad=0.0000), Var(v=-0.0121, grad=0.0000), Var(v=0.1296, grad=0.0000), Var(v=-0.1515, grad=-2.2894), Var(v=-0.1179, grad=1.6317), Var(v=0.1353, grad=0.0000), Var(v=-0.3063, grad=0.0000), Var(v=0.0609, grad=0.5255), Var(v=-0.0532, grad=0.0000), Var(v=0.1516, grad=-0.3052), Var(v=0.1072, grad=-1.5227), Var(v=-0.2133, grad=0.0000), Var(v=0.0514, grad=0.0000), Var(v=-0.0289, grad=0.0000), Var(v=-0.2248, grad=0.0000), Var(v=0.0461, grad=0.0000), Var(v=-0.0302, grad=0.3276), Var(v=0.0525, grad=0.3497), Var(v=-0.0450, grad=0.0000), Var(v=-0.0624, grad=0.0000), Var(v=0.0849, grad=-1.5494), Var(v=0.0424, grad=-0.0842), Var(v=0.0392, grad=-1.0362), Var(v=0.0031, grad=-1.1263)], [Var(v=0.0584, grad=-0.5022), Var(v=0.0917, grad=0.0000), Var(v=0.0476, grad=0.5980), Var(v=0.1390, grad=-2.3098), Var(v=-0.0154, grad=0.0000), Var(v=-0.1351, grad=0.0000), Var(v=0.1509, grad=0.3806), Var(v=0.1654, grad=-0.9212), Var(v=-0.1658, grad=0.0000), Var(v=-0.2197, grad=0.4588), Var(v=-0.0155, grad=0.0725), Var(v=-0.1135, grad=0.0000), Var(v=-0.1935, grad=0.0000), Var(v=0.0483, grad=0.0000), Var(v=-0.0971, grad=0.0000), Var(v=-0.1751, grad=0.0000), Var(v=-0.0424, grad=0.0000), Var(v=-0.0428, grad=0.7257), Var(v=0.2091, grad=0.2203), Var(v=-0.1110, grad=0.0000), Var(v=0.0075, grad=0.0000), Var(v=0.0712, grad=1.2310), Var(v=0.1351, grad=0.0178), Var(v=-0.0624, grad=0.0000), Var(v=0.1825, grad=0.0000), Var(v=-0.0304, grad=0.0000), Var(v=0.0346, grad=0.0000), Var(v=-0.1420, grad=0.0000), Var(v=-0.0140, grad=0.0000), Var(v=0.0124, grad=-1.8387), Var(v=-0.0159, grad=1.3105), Var(v=-0.1047, grad=0.0000), Var(v=0.0906, grad=0.0000), Var(v=0.0362, grad=0.4221), Var(v=-0.0606, grad=0.0000), Var(v=-0.0730, grad=-0.2451), Var(v=0.1020, grad=-1.2229), Var(v=-0.0258, grad=0.0000), Var(v=-0.0365, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=-0.2374, grad=0.0000), Var(v=0.0131, grad=0.0000), Var(v=-0.0267, grad=0.2631), Var(v=-0.1470, grad=0.2809), Var(v=-0.1482, grad=0.0000), Var(v=0.0182, grad=0.0000), Var(v=-0.1120, grad=-1.2444), Var(v=0.1016, grad=-0.0676), Var(v=0.0931, grad=-0.8322), Var(v=0.1261, grad=-0.9046)], [Var(v=-0.1426, grad=0.0000), Var(v=-0.0933, grad=0.4727), Var(v=0.1264, grad=0.0000), Var(v=-0.0269, grad=1.2107), Var(v=0.1397, grad=0.1907), Var(v=0.0151, grad=0.0000), Var(v=0.0394, grad=0.0000), Var(v=0.0900, grad=0.4828), Var(v=0.0323, grad=0.0000), Var(v=0.1841, grad=0.0000), Var(v=0.2547, grad=-0.0380), Var(v=-0.2196, grad=0.0000), Var(v=0.1067, grad=-0.5834), Var(v=-0.2187, grad=0.0000), Var(v=-0.0582, grad=0.0000), Var(v=0.2172, grad=-0.4293), Var(v=0.0281, grad=-0.0849), Var(v=-0.1337, grad=-0.3804), Var(v=0.0508, grad=0.0000), Var(v=0.0743, grad=-1.2381), Var(v=0.1033, grad=-0.1984), Var(v=0.0939, grad=-0.6452), Var(v=0.1118, grad=-0.0093), Var(v=-0.0352, grad=-0.2922), Var(v=0.0066, grad=0.0000), Var(v=0.0766, grad=0.6181), Var(v=-0.1084, grad=0.0000), Var(v=-0.1309, grad=0.2250), Var(v=-0.1987, grad=0.3634), Var(v=-0.2852, grad=0.9638), Var(v=-0.0485, grad=0.0000), Var(v=0.2989, grad=0.0366), Var(v=0.0259, grad=-0.7273), Var(v=0.0190, grad=-0.2212), Var(v=0.0646, grad=0.0030), Var(v=-0.0676, grad=0.0000), Var(v=0.0175, grad=0.6410), Var(v=0.0146, grad=0.0000), Var(v=0.0524, grad=0.6212), Var(v=-0.0307, grad=0.0000), Var(v=0.0720, grad=0.0688), Var(v=0.0673, grad=0.0000), Var(v=0.0497, grad=0.0000), Var(v=-0.0441, grad=0.0000), Var(v=0.0238, grad=0.3306), Var(v=0.0905, grad=0.0000), Var(v=0.0167, grad=0.0000), Var(v=-0.1544, grad=0.0000), Var(v=0.0028, grad=0.0000), Var(v=-0.0946, grad=0.4741)], [Var(v=-0.0753, grad=-0.1897), Var(v=-0.1132, grad=0.0000), Var(v=0.0495, grad=0.2259), Var(v=0.0875, grad=-0.8724), Var(v=-0.0670, grad=0.0000), Var(v=0.0840, grad=0.0000), Var(v=-0.0314, grad=0.1438), Var(v=-0.0944, grad=-0.3479), Var(v=-0.1870, grad=0.0000), Var(v=0.0411, grad=0.1733), Var(v=-0.0795, grad=0.0274), Var(v=0.1164, grad=0.0000), Var(v=0.0526, grad=0.0000), Var(v=-0.0680, grad=0.0000), Var(v=-0.0784, grad=0.0000), Var(v=-0.0779, grad=0.0000), Var(v=0.1171, grad=0.0000), Var(v=-0.1213, grad=0.2741), Var(v=0.0811, grad=0.0832), Var(v=-0.1726, grad=0.0000), Var(v=-0.1346, grad=0.0000), Var(v=-0.0194, grad=0.4650), Var(v=0.0619, grad=0.0067), Var(v=0.1117, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=-0.0416, grad=0.0000), Var(v=-0.0690, grad=0.0000), Var(v=0.0169, grad=0.0000), Var(v=-0.0663, grad=0.0000), Var(v=0.1403, grad=-0.6945), Var(v=-0.1084, grad=0.4950), Var(v=0.0308, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=0.0593, grad=0.1594), Var(v=0.1364, grad=0.0000), Var(v=-0.1611, grad=-0.0926), Var(v=-0.0232, grad=-0.4619), Var(v=0.0045, grad=0.0000), Var(v=0.0505, grad=0.0000), Var(v=-0.0200, grad=0.0000), Var(v=-0.0070, grad=0.0000), Var(v=-0.1070, grad=0.0000), Var(v=0.0643, grad=0.0994), Var(v=-0.0279, grad=0.1061), Var(v=0.0343, grad=0.0000), Var(v=0.0972, grad=0.0000), Var(v=0.0833, grad=-0.4700), Var(v=0.0465, grad=-0.0256), Var(v=-0.0727, grad=-0.3143), Var(v=-0.1025, grad=-0.3417)]] Biases: [Var(v=0.0488, grad=-4.8775), Var(v=-0.0900, grad=8.9951), Var(v=-0.0581, grad=5.8072), Var(v=-0.0061, grad=0.6072), Var(v=-0.0363, grad=3.6283), Var(v=0.0000, grad=0.0000), Var(v=-0.0370, grad=3.6963), Var(v=-0.0024, grad=0.2422), Var(v=0.0000, grad=0.0000), Var(v=-0.0446, grad=4.4555), Var(v=0.0002, grad=-0.0190), Var(v=0.0000, grad=0.0000), Var(v=0.1110, grad=-11.1012), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0817, grad=-8.1691), Var(v=0.0162, grad=-1.6163), Var(v=0.0019, grad=-0.1908), Var(v=-0.0214, grad=2.1394), Var(v=0.2356, grad=-23.5608), Var(v=0.0378, grad=-3.7756), Var(v=0.0032, grad=-0.3236), Var(v=0.0000, grad=-0.0047), Var(v=0.0556, grad=-5.5612), Var(v=0.0000, grad=0.0000), Var(v=-0.1176, grad=11.7632), Var(v=0.0000, grad=0.0000), Var(v=-0.0428, grad=4.2816), Var(v=-0.0691, grad=6.9148), Var(v=-0.0048, grad=0.4834), Var(v=-0.1273, grad=12.7266), Var(v=-0.0070, grad=0.6963), Var(v=0.1384, grad=-13.8402), Var(v=0.0011, grad=-0.1110), Var(v=-0.0006, grad=0.0569), Var(v=0.0238, grad=-2.3802), Var(v=-0.0032, grad=0.3215), Var(v=0.0000, grad=0.0000), Var(v=-0.1182, grad=11.8214), Var(v=0.0000, grad=0.0000), Var(v=-0.0131, grad=1.3102), Var(v=0.0000, grad=0.0000), Var(v=-0.0256, grad=2.5555), Var(v=-0.0273, grad=2.7277), Var(v=-0.0629, grad=6.2907), Var(v=0.0000, grad=0.0000), Var(v=0.1209, grad=-12.0851), Var(v=0.0066, grad=-0.6569), Var(v=0.0808, grad=-8.0820), Var(v=-0.0024, grad=0.2378)]\n",
            "Layer 2 \n",
            " Weights: [[Var(v=-0.0839, grad=2.6895)], [Var(v=-0.1018, grad=-0.0612)], [Var(v=0.0470, grad=2.0915)], [Var(v=-0.2679, grad=0.5607)], [Var(v=-0.0171, grad=-2.4227)], [Var(v=-0.1486, grad=0.0000)], [Var(v=0.0107, grad=3.2507)], [Var(v=-0.1330, grad=2.8451)], [Var(v=-0.0833, grad=0.0000)], [Var(v=0.0331, grad=1.9005)], [Var(v=0.0271, grad=-1.8894)], [Var(v=0.0255, grad=0.0000)], [Var(v=0.1540, grad=-2.7591)], [Var(v=-0.2298, grad=0.0000)], [Var(v=-0.1519, grad=0.0000)], [Var(v=0.1093, grad=-1.6287)], [Var(v=0.0282, grad=-0.9769)], [Var(v=0.0809, grad=0.1473)], [Var(v=-0.0111, grad=3.6075)], [Var(v=0.2852, grad=-1.7029)], [Var(v=0.0496, grad=-0.6615)], [Var(v=0.1493, grad=-0.9493)], [Var(v=-0.0069, grad=0.8877)], [Var(v=0.0650, grad=-0.1709)], [Var(v=-0.0502, grad=0.0000)], [Var(v=-0.1017, grad=-3.2242)], [Var(v=0.0748, grad=0.0000)], [Var(v=-0.0394, grad=-0.9311)], [Var(v=-0.0668, grad=-1.1864)], [Var(v=-0.2182, grad=0.9448)], [Var(v=0.1453, grad=0.3536)], [Var(v=0.0011, grad=-0.8981)], [Var(v=0.1734, grad=-1.5844)], [Var(v=0.0568, grad=-0.8917)], [Var(v=0.0181, grad=-1.8743)], [Var(v=-0.0348, grad=0.7007)], [Var(v=-0.1454, grad=0.6573)], [Var(v=-0.0316, grad=0.0000)], [Var(v=-0.1235, grad=-1.1015)], [Var(v=-0.0368, grad=0.0000)], [Var(v=0.0010, grad=-1.5878)], [Var(v=0.1221, grad=0.0000)], [Var(v=0.0096, grad=2.0252)], [Var(v=-0.0124, grad=4.4336)], [Var(v=-0.0331, grad=-3.8468)], [Var(v=0.1181, grad=0.0000)], [Var(v=-0.1624, grad=2.1066)], [Var(v=-0.0080, grad=0.0305)], [Var(v=-0.1281, grad=3.3618)], [Var(v=-0.1041, grad=0.1399)]] Biases: [Var(v=0.0232, grad=-2.3154)]\n",
            "\n",
            "Network after zeroing gradients:\n",
            "Layer 0 \n",
            " Weights: [[Var(v=-0.0166, grad=0.0000), Var(v=-0.1257, grad=0.0000), Var(v=-0.1512, grad=0.0000), Var(v=-0.0235, grad=0.0000), Var(v=-0.1133, grad=0.0000), Var(v=0.0100, grad=0.0000), Var(v=0.0134, grad=0.0000), Var(v=-0.0174, grad=0.0000), Var(v=-0.2474, grad=0.0000), Var(v=0.0783, grad=0.0000), Var(v=0.0901, grad=0.0000), Var(v=-0.1371, grad=0.0000), Var(v=-0.1431, grad=0.0000), Var(v=0.1642, grad=0.0000), Var(v=-0.0693, grad=0.0000)]] Biases: [Var(v=0.0121, grad=0.0000), Var(v=-0.0026, grad=0.0000), Var(v=0.0759, grad=0.0000), Var(v=-0.0296, grad=0.0000), Var(v=0.0726, grad=0.0000), Var(v=-0.0033, grad=0.0000), Var(v=-0.0219, grad=0.0000), Var(v=-0.0141, grad=0.0000), Var(v=0.0533, grad=0.0000), Var(v=-0.1286, grad=0.0000), Var(v=-0.0608, grad=0.0000), Var(v=0.0188, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=0.1022, grad=0.0000), Var(v=0.0309, grad=0.0000)]\n",
            "Layer 1 \n",
            " Weights: [[Var(v=-0.0341, grad=0.0000), Var(v=0.2596, grad=0.0000), Var(v=0.0326, grad=0.0000), Var(v=0.0805, grad=0.0000), Var(v=0.1167, grad=0.0000), Var(v=0.1777, grad=0.0000), Var(v=-0.1693, grad=0.0000), Var(v=-0.0881, grad=0.0000), Var(v=-0.0355, grad=0.0000), Var(v=-0.1079, grad=0.0000), Var(v=0.1215, grad=0.0000), Var(v=-0.1300, grad=0.0000), Var(v=0.0500, grad=0.0000), Var(v=-0.0657, grad=0.0000), Var(v=-0.1067, grad=0.0000), Var(v=0.0295, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.2482, grad=0.0000), Var(v=0.1019, grad=0.0000), Var(v=0.0111, grad=0.0000), Var(v=-0.1020, grad=0.0000), Var(v=-0.0077, grad=0.0000), Var(v=-0.0215, grad=0.0000), Var(v=-0.0346, grad=0.0000), Var(v=-0.0588, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=-0.0412, grad=0.0000), Var(v=-0.0345, grad=0.0000), Var(v=-0.0248, grad=0.0000), Var(v=-0.0757, grad=0.0000), Var(v=-0.0290, grad=0.0000), Var(v=-0.1563, grad=0.0000), Var(v=0.0072, grad=0.0000), Var(v=-0.1274, grad=0.0000), Var(v=0.0047, grad=0.0000), Var(v=0.3267, grad=0.0000), Var(v=-0.0204, grad=0.0000), Var(v=0.1360, grad=0.0000), Var(v=-0.1031, grad=0.0000), Var(v=-0.0361, grad=0.0000), Var(v=-0.1102, grad=0.0000), Var(v=-0.0052, grad=0.0000), Var(v=-0.0488, grad=0.0000), Var(v=0.0106, grad=0.0000), Var(v=0.0507, grad=0.0000), Var(v=-0.1748, grad=0.0000), Var(v=-0.0909, grad=0.0000), Var(v=0.0416, grad=0.0000), Var(v=0.0623, grad=0.0000), Var(v=-0.1864, grad=0.0000)], [Var(v=-0.0725, grad=0.0000), Var(v=-0.0818, grad=0.0000), Var(v=-0.0426, grad=0.0000), Var(v=0.0298, grad=0.0000), Var(v=-0.2672, grad=0.0000), Var(v=-0.0861, grad=0.0000), Var(v=0.0647, grad=0.0000), Var(v=0.0347, grad=0.0000), Var(v=-0.1109, grad=0.0000), Var(v=-0.0367, grad=0.0000), Var(v=0.0432, grad=0.0000), Var(v=0.0006, grad=0.0000), Var(v=-0.0567, grad=0.0000), Var(v=-0.0825, grad=0.0000), Var(v=-0.1025, grad=0.0000), Var(v=0.0132, grad=0.0000), Var(v=-0.0494, grad=0.0000), Var(v=0.1058, grad=0.0000), Var(v=0.1442, grad=0.0000), Var(v=-0.0520, grad=0.0000), Var(v=-0.1259, grad=0.0000), Var(v=0.0338, grad=0.0000), Var(v=0.0808, grad=0.0000), Var(v=0.0061, grad=0.0000), Var(v=-0.0026, grad=0.0000), Var(v=-0.0059, grad=0.0000), Var(v=0.1792, grad=0.0000), Var(v=0.0162, grad=0.0000), Var(v=-0.0467, grad=0.0000), Var(v=0.0806, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=-0.1821, grad=0.0000), Var(v=-0.0426, grad=0.0000), Var(v=-0.1496, grad=0.0000), Var(v=-0.1210, grad=0.0000), Var(v=-0.0839, grad=0.0000), Var(v=0.0547, grad=0.0000), Var(v=0.0778, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=0.0102, grad=0.0000), Var(v=0.1392, grad=0.0000), Var(v=-0.0149, grad=0.0000), Var(v=-0.0325, grad=0.0000), Var(v=0.2621, grad=0.0000), Var(v=-0.0205, grad=0.0000), Var(v=0.0265, grad=0.0000), Var(v=0.0464, grad=0.0000), Var(v=-0.1443, grad=0.0000), Var(v=0.0139, grad=0.0000), Var(v=0.0635, grad=0.0000)], [Var(v=0.0368, grad=0.0000), Var(v=-0.0234, grad=0.0000), Var(v=0.0001, grad=0.0000), Var(v=0.0222, grad=0.0000), Var(v=-0.0649, grad=0.0000), Var(v=-0.0680, grad=0.0000), Var(v=-0.0007, grad=0.0000), Var(v=0.1833, grad=0.0000), Var(v=0.0335, grad=0.0000), Var(v=-0.0854, grad=0.0000), Var(v=-0.0116, grad=0.0000), Var(v=0.0064, grad=0.0000), Var(v=0.0032, grad=0.0000), Var(v=-0.1744, grad=0.0000), Var(v=-0.0852, grad=0.0000), Var(v=0.2706, grad=0.0000), Var(v=-0.0491, grad=0.0000), Var(v=-0.0286, grad=0.0000), Var(v=-0.0851, grad=0.0000), Var(v=-0.0663, grad=0.0000), Var(v=0.1334, grad=0.0000), Var(v=-0.0008, grad=0.0000), Var(v=-0.0027, grad=0.0000), Var(v=-0.0791, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=0.1167, grad=0.0000), Var(v=-0.1581, grad=0.0000), Var(v=-0.1009, grad=0.0000), Var(v=-0.1836, grad=0.0000), Var(v=0.0005, grad=0.0000), Var(v=-0.0342, grad=0.0000), Var(v=-0.0561, grad=0.0000), Var(v=0.1121, grad=0.0000), Var(v=-0.0656, grad=0.0000), Var(v=-0.0781, grad=0.0000), Var(v=0.0029, grad=0.0000), Var(v=0.1593, grad=0.0000), Var(v=0.0165, grad=0.0000), Var(v=0.0951, grad=0.0000), Var(v=0.0620, grad=0.0000), Var(v=0.0442, grad=0.0000), Var(v=-0.1939, grad=0.0000), Var(v=-0.0445, grad=0.0000), Var(v=-0.0518, grad=0.0000), Var(v=-0.0578, grad=0.0000), Var(v=-0.1415, grad=0.0000), Var(v=0.0882, grad=0.0000), Var(v=0.0234, grad=0.0000), Var(v=0.1213, grad=0.0000), Var(v=0.1039, grad=0.0000)], [Var(v=0.0290, grad=0.0000), Var(v=0.1867, grad=0.0000), Var(v=-0.0621, grad=0.0000), Var(v=0.0246, grad=0.0000), Var(v=0.0335, grad=0.0000), Var(v=-0.0643, grad=0.0000), Var(v=0.0410, grad=0.0000), Var(v=-0.0672, grad=0.0000), Var(v=-0.1584, grad=0.0000), Var(v=0.1027, grad=0.0000), Var(v=0.1067, grad=0.0000), Var(v=0.1469, grad=0.0000), Var(v=-0.1108, grad=0.0000), Var(v=-0.0054, grad=0.0000), Var(v=0.1254, grad=0.0000), Var(v=0.0641, grad=0.0000), Var(v=0.1676, grad=0.0000), Var(v=0.0887, grad=0.0000), Var(v=0.1521, grad=0.0000), Var(v=-0.0560, grad=0.0000), Var(v=-0.0025, grad=0.0000), Var(v=0.0443, grad=0.0000), Var(v=-0.0029, grad=0.0000), Var(v=0.0516, grad=0.0000), Var(v=0.0687, grad=0.0000), Var(v=0.0408, grad=0.0000), Var(v=0.0308, grad=0.0000), Var(v=0.0746, grad=0.0000), Var(v=-0.0269, grad=0.0000), Var(v=0.1432, grad=0.0000), Var(v=-0.0371, grad=0.0000), Var(v=-0.2732, grad=0.0000), Var(v=0.0797, grad=0.0000), Var(v=0.0809, grad=0.0000), Var(v=0.1249, grad=0.0000), Var(v=-0.0792, grad=0.0000), Var(v=0.0541, grad=0.0000), Var(v=-0.0120, grad=0.0000), Var(v=-0.0843, grad=0.0000), Var(v=-0.0099, grad=0.0000), Var(v=-0.0561, grad=0.0000), Var(v=0.0133, grad=0.0000), Var(v=0.0104, grad=0.0000), Var(v=0.0865, grad=0.0000), Var(v=0.0361, grad=0.0000), Var(v=0.1116, grad=0.0000), Var(v=0.0766, grad=0.0000), Var(v=0.0337, grad=0.0000), Var(v=-0.1785, grad=0.0000), Var(v=-0.0889, grad=0.0000)], [Var(v=0.0841, grad=0.0000), Var(v=-0.0541, grad=0.0000), Var(v=-0.1000, grad=0.0000), Var(v=0.0349, grad=0.0000), Var(v=0.0059, grad=0.0000), Var(v=-0.1164, grad=0.0000), Var(v=-0.1089, grad=0.0000), Var(v=-0.1272, grad=0.0000), Var(v=0.0157, grad=0.0000), Var(v=-0.0177, grad=0.0000), Var(v=-0.0297, grad=0.0000), Var(v=-0.0993, grad=0.0000), Var(v=-0.2383, grad=0.0000), Var(v=0.1590, grad=0.0000), Var(v=0.2111, grad=0.0000), Var(v=-0.1469, grad=0.0000), Var(v=-0.0489, grad=0.0000), Var(v=0.1159, grad=0.0000), Var(v=0.0043, grad=0.0000), Var(v=-0.0924, grad=0.0000), Var(v=-0.1577, grad=0.0000), Var(v=-0.1021, grad=0.0000), Var(v=0.1529, grad=0.0000), Var(v=0.1494, grad=0.0000), Var(v=-0.2201, grad=0.0000), Var(v=0.2262, grad=0.0000), Var(v=0.0505, grad=0.0000), Var(v=0.1703, grad=0.0000), Var(v=-0.1734, grad=0.0000), Var(v=0.1317, grad=0.0000), Var(v=-0.0117, grad=0.0000), Var(v=-0.0267, grad=0.0000), Var(v=0.0440, grad=0.0000), Var(v=-0.0099, grad=0.0000), Var(v=0.0900, grad=0.0000), Var(v=-0.0126, grad=0.0000), Var(v=0.2534, grad=0.0000), Var(v=-0.0450, grad=0.0000), Var(v=0.0546, grad=0.0000), Var(v=0.0519, grad=0.0000), Var(v=-0.0788, grad=0.0000), Var(v=0.1319, grad=0.0000), Var(v=0.0160, grad=0.0000), Var(v=0.1305, grad=0.0000), Var(v=-0.0050, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1565, grad=0.0000), Var(v=0.1205, grad=0.0000), Var(v=-0.0566, grad=0.0000), Var(v=-0.0242, grad=0.0000)], [Var(v=-0.0058, grad=0.0000), Var(v=-0.0261, grad=0.0000), Var(v=-0.0979, grad=0.0000), Var(v=0.1683, grad=0.0000), Var(v=0.0502, grad=0.0000), Var(v=0.0093, grad=0.0000), Var(v=-0.0094, grad=0.0000), Var(v=0.0543, grad=0.0000), Var(v=-0.0742, grad=0.0000), Var(v=0.0111, grad=0.0000), Var(v=0.0708, grad=0.0000), Var(v=0.0641, grad=0.0000), Var(v=0.0099, grad=0.0000), Var(v=-0.0334, grad=0.0000), Var(v=-0.0411, grad=0.0000), Var(v=0.1550, grad=0.0000), Var(v=0.1720, grad=0.0000), Var(v=0.0512, grad=0.0000), Var(v=-0.0394, grad=0.0000), Var(v=0.1258, grad=0.0000), Var(v=-0.0241, grad=0.0000), Var(v=0.0236, grad=0.0000), Var(v=0.1799, grad=0.0000), Var(v=-0.0641, grad=0.0000), Var(v=0.0865, grad=0.0000), Var(v=-0.0041, grad=0.0000), Var(v=0.0953, grad=0.0000), Var(v=-0.0010, grad=0.0000), Var(v=0.0632, grad=0.0000), Var(v=-0.0950, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=-0.0938, grad=0.0000), Var(v=-0.1164, grad=0.0000), Var(v=0.0966, grad=0.0000), Var(v=0.0218, grad=0.0000), Var(v=-0.0063, grad=0.0000), Var(v=-0.1634, grad=0.0000), Var(v=-0.3484, grad=0.0000), Var(v=0.0696, grad=0.0000), Var(v=0.0073, grad=0.0000), Var(v=-0.0977, grad=0.0000), Var(v=-0.2139, grad=0.0000), Var(v=-0.1293, grad=0.0000), Var(v=0.0286, grad=0.0000), Var(v=0.1947, grad=0.0000), Var(v=-0.1361, grad=0.0000), Var(v=0.0635, grad=0.0000), Var(v=0.0862, grad=0.0000), Var(v=0.0302, grad=0.0000), Var(v=0.0835, grad=0.0000)], [Var(v=-0.1527, grad=0.0000), Var(v=-0.0536, grad=0.0000), Var(v=0.0475, grad=0.0000), Var(v=-0.0540, grad=0.0000), Var(v=-0.0403, grad=0.0000), Var(v=-0.1175, grad=0.0000), Var(v=-0.1350, grad=0.0000), Var(v=0.0952, grad=0.0000), Var(v=0.1426, grad=0.0000), Var(v=0.0504, grad=0.0000), Var(v=-0.1053, grad=0.0000), Var(v=0.0234, grad=0.0000), Var(v=0.0223, grad=0.0000), Var(v=0.1290, grad=0.0000), Var(v=0.1047, grad=0.0000), Var(v=-0.0152, grad=0.0000), Var(v=-0.1527, grad=0.0000), Var(v=-0.1212, grad=0.0000), Var(v=-0.1297, grad=0.0000), Var(v=-0.0739, grad=0.0000), Var(v=-0.0619, grad=0.0000), Var(v=-0.0052, grad=0.0000), Var(v=0.0260, grad=0.0000), Var(v=0.0766, grad=0.0000), Var(v=0.0633, grad=0.0000), Var(v=0.0898, grad=0.0000), Var(v=0.0056, grad=0.0000), Var(v=0.1841, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=0.0521, grad=0.0000), Var(v=0.0073, grad=0.0000), Var(v=-0.0227, grad=0.0000), Var(v=-0.1125, grad=0.0000), Var(v=0.0433, grad=0.0000), Var(v=-0.0978, grad=0.0000), Var(v=0.0362, grad=0.0000), Var(v=-0.1408, grad=0.0000), Var(v=-0.0107, grad=0.0000), Var(v=0.0673, grad=0.0000), Var(v=-0.1039, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=0.0755, grad=0.0000), Var(v=0.1633, grad=0.0000), Var(v=0.1257, grad=0.0000), Var(v=-0.1480, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=-0.0450, grad=0.0000), Var(v=-0.0535, grad=0.0000), Var(v=-0.0243, grad=0.0000), Var(v=0.0374, grad=0.0000)], [Var(v=0.1507, grad=0.0000), Var(v=0.1051, grad=0.0000), Var(v=-0.1046, grad=0.0000), Var(v=0.0602, grad=0.0000), Var(v=-0.0034, grad=0.0000), Var(v=-0.2291, grad=0.0000), Var(v=-0.0020, grad=0.0000), Var(v=-0.0003, grad=0.0000), Var(v=-0.0572, grad=0.0000), Var(v=-0.0726, grad=0.0000), Var(v=-0.0588, grad=0.0000), Var(v=-0.2074, grad=0.0000), Var(v=-0.0407, grad=0.0000), Var(v=-0.1832, grad=0.0000), Var(v=-0.0378, grad=0.0000), Var(v=-0.0342, grad=0.0000), Var(v=-0.0596, grad=0.0000), Var(v=0.0407, grad=0.0000), Var(v=0.0773, grad=0.0000), Var(v=-0.0195, grad=0.0000), Var(v=-0.2161, grad=0.0000), Var(v=-0.0108, grad=0.0000), Var(v=0.0596, grad=0.0000), Var(v=-0.1000, grad=0.0000), Var(v=0.1419, grad=0.0000), Var(v=0.0474, grad=0.0000), Var(v=-0.0368, grad=0.0000), Var(v=-0.0547, grad=0.0000), Var(v=-0.0211, grad=0.0000), Var(v=0.0439, grad=0.0000), Var(v=-0.0135, grad=0.0000), Var(v=-0.0830, grad=0.0000), Var(v=0.0054, grad=0.0000), Var(v=0.0021, grad=0.0000), Var(v=-0.0677, grad=0.0000), Var(v=0.0183, grad=0.0000), Var(v=-0.1169, grad=0.0000), Var(v=-0.2066, grad=0.0000), Var(v=-0.1513, grad=0.0000), Var(v=0.0155, grad=0.0000), Var(v=-0.1155, grad=0.0000), Var(v=0.0592, grad=0.0000), Var(v=0.0310, grad=0.0000), Var(v=-0.0149, grad=0.0000), Var(v=-0.0470, grad=0.0000), Var(v=0.1156, grad=0.0000), Var(v=-0.1826, grad=0.0000), Var(v=-0.0715, grad=0.0000), Var(v=-0.1218, grad=0.0000), Var(v=0.0223, grad=0.0000)], [Var(v=0.0672, grad=0.0000), Var(v=-0.0124, grad=0.0000), Var(v=0.0739, grad=0.0000), Var(v=0.1125, grad=0.0000), Var(v=0.0741, grad=0.0000), Var(v=0.0470, grad=0.0000), Var(v=0.0169, grad=0.0000), Var(v=0.1287, grad=0.0000), Var(v=-0.1036, grad=0.0000), Var(v=0.2044, grad=0.0000), Var(v=0.0734, grad=0.0000), Var(v=-0.0794, grad=0.0000), Var(v=-0.0337, grad=0.0000), Var(v=-0.0645, grad=0.0000), Var(v=-0.0461, grad=0.0000), Var(v=-0.0137, grad=0.0000), Var(v=-0.0253, grad=0.0000), Var(v=0.1143, grad=0.0000), Var(v=-0.0241, grad=0.0000), Var(v=-0.0317, grad=0.0000), Var(v=-0.0496, grad=0.0000), Var(v=-0.1192, grad=0.0000), Var(v=-0.0087, grad=0.0000), Var(v=0.0987, grad=0.0000), Var(v=-0.0086, grad=0.0000), Var(v=-0.1868, grad=0.0000), Var(v=-0.1444, grad=0.0000), Var(v=-0.2083, grad=0.0000), Var(v=-0.0356, grad=0.0000), Var(v=0.1600, grad=0.0000), Var(v=0.0636, grad=0.0000), Var(v=-0.0216, grad=0.0000), Var(v=-0.0268, grad=0.0000), Var(v=0.1238, grad=0.0000), Var(v=-0.1733, grad=0.0000), Var(v=0.0808, grad=0.0000), Var(v=0.0369, grad=0.0000), Var(v=-0.0662, grad=0.0000), Var(v=-0.1196, grad=0.0000), Var(v=-0.0838, grad=0.0000), Var(v=-0.1491, grad=0.0000), Var(v=-0.0845, grad=0.0000), Var(v=0.1100, grad=0.0000), Var(v=0.0883, grad=0.0000), Var(v=-0.1012, grad=0.0000), Var(v=-0.0443, grad=0.0000), Var(v=0.1622, grad=0.0000), Var(v=0.0031, grad=0.0000), Var(v=0.1788, grad=0.0000), Var(v=0.0947, grad=0.0000)], [Var(v=-0.0191, grad=0.0000), Var(v=0.1257, grad=0.0000), Var(v=0.0693, grad=0.0000), Var(v=-0.0261, grad=0.0000), Var(v=-0.1021, grad=0.0000), Var(v=-0.0466, grad=0.0000), Var(v=0.1139, grad=0.0000), Var(v=-0.0295, grad=0.0000), Var(v=0.0900, grad=0.0000), Var(v=0.0499, grad=0.0000), Var(v=0.1699, grad=0.0000), Var(v=-0.0953, grad=0.0000), Var(v=-0.0705, grad=0.0000), Var(v=0.0412, grad=0.0000), Var(v=0.0193, grad=0.0000), Var(v=-0.1945, grad=0.0000), Var(v=-0.0476, grad=0.0000), Var(v=0.0984, grad=0.0000), Var(v=0.1492, grad=0.0000), Var(v=-0.0862, grad=0.0000), Var(v=0.2004, grad=0.0000), Var(v=0.0917, grad=0.0000), Var(v=0.1197, grad=0.0000), Var(v=-0.1280, grad=0.0000), Var(v=-0.0394, grad=0.0000), Var(v=-0.0353, grad=0.0000), Var(v=0.0256, grad=0.0000), Var(v=0.1017, grad=0.0000), Var(v=0.0163, grad=0.0000), Var(v=-0.1096, grad=0.0000), Var(v=0.0319, grad=0.0000), Var(v=0.0182, grad=0.0000), Var(v=-0.1387, grad=0.0000), Var(v=-0.0481, grad=0.0000), Var(v=0.0962, grad=0.0000), Var(v=-0.0274, grad=0.0000), Var(v=-0.1455, grad=0.0000), Var(v=-0.0200, grad=0.0000), Var(v=-0.0232, grad=0.0000), Var(v=-0.0005, grad=0.0000), Var(v=-0.1554, grad=0.0000), Var(v=0.1371, grad=0.0000), Var(v=0.0964, grad=0.0000), Var(v=0.0166, grad=0.0000), Var(v=0.0323, grad=0.0000), Var(v=-0.0431, grad=0.0000), Var(v=-0.1496, grad=0.0000), Var(v=0.0001, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=-0.0735, grad=0.0000)], [Var(v=-0.1602, grad=0.0000), Var(v=0.0010, grad=0.0000), Var(v=-0.0804, grad=0.0000), Var(v=0.0615, grad=0.0000), Var(v=0.0940, grad=0.0000), Var(v=-0.0453, grad=0.0000), Var(v=-0.1003, grad=0.0000), Var(v=0.0444, grad=0.0000), Var(v=-0.1378, grad=0.0000), Var(v=-0.0973, grad=0.0000), Var(v=0.1368, grad=0.0000), Var(v=0.0059, grad=0.0000), Var(v=0.1672, grad=0.0000), Var(v=-0.1167, grad=0.0000), Var(v=-0.0093, grad=0.0000), Var(v=0.0370, grad=0.0000), Var(v=0.0285, grad=0.0000), Var(v=0.1665, grad=0.0000), Var(v=-0.0975, grad=0.0000), Var(v=0.1237, grad=0.0000), Var(v=0.0212, grad=0.0000), Var(v=0.0557, grad=0.0000), Var(v=0.1219, grad=0.0000), Var(v=0.0336, grad=0.0000), Var(v=-0.0544, grad=0.0000), Var(v=0.1481, grad=0.0000), Var(v=-0.0900, grad=0.0000), Var(v=0.0796, grad=0.0000), Var(v=0.1102, grad=0.0000), Var(v=0.0480, grad=0.0000), Var(v=-0.0037, grad=0.0000), Var(v=-0.0039, grad=0.0000), Var(v=0.1219, grad=0.0000), Var(v=0.1176, grad=0.0000), Var(v=0.0853, grad=0.0000), Var(v=-0.0215, grad=0.0000), Var(v=0.1039, grad=0.0000), Var(v=-0.0217, grad=0.0000), Var(v=0.0279, grad=0.0000), Var(v=-0.0989, grad=0.0000), Var(v=0.0859, grad=0.0000), Var(v=-0.1231, grad=0.0000), Var(v=-0.0682, grad=0.0000), Var(v=-0.0814, grad=0.0000), Var(v=0.1951, grad=0.0000), Var(v=-0.0701, grad=0.0000), Var(v=-0.0328, grad=0.0000), Var(v=-0.0358, grad=0.0000), Var(v=-0.0930, grad=0.0000), Var(v=0.1605, grad=0.0000)], [Var(v=0.1694, grad=0.0000), Var(v=-0.0081, grad=0.0000), Var(v=0.0325, grad=0.0000), Var(v=0.0340, grad=0.0000), Var(v=0.0544, grad=0.0000), Var(v=0.0847, grad=0.0000), Var(v=0.0433, grad=0.0000), Var(v=0.0541, grad=0.0000), Var(v=0.1040, grad=0.0000), Var(v=0.0567, grad=0.0000), Var(v=-0.0318, grad=0.0000), Var(v=0.0324, grad=0.0000), Var(v=-0.1673, grad=0.0000), Var(v=0.1526, grad=0.0000), Var(v=0.0984, grad=0.0000), Var(v=0.1112, grad=0.0000), Var(v=-0.0370, grad=0.0000), Var(v=-0.1513, grad=0.0000), Var(v=-0.0467, grad=0.0000), Var(v=0.1197, grad=0.0000), Var(v=-0.0249, grad=0.0000), Var(v=-0.0031, grad=0.0000), Var(v=0.0039, grad=0.0000), Var(v=-0.1564, grad=0.0000), Var(v=-0.2191, grad=0.0000), Var(v=-0.1411, grad=0.0000), Var(v=-0.0745, grad=0.0000), Var(v=-0.0121, grad=0.0000), Var(v=0.1296, grad=0.0000), Var(v=-0.1515, grad=0.0000), Var(v=-0.1179, grad=0.0000), Var(v=0.1353, grad=0.0000), Var(v=-0.3063, grad=0.0000), Var(v=0.0609, grad=0.0000), Var(v=-0.0532, grad=0.0000), Var(v=0.1516, grad=0.0000), Var(v=0.1072, grad=0.0000), Var(v=-0.2133, grad=0.0000), Var(v=0.0514, grad=0.0000), Var(v=-0.0289, grad=0.0000), Var(v=-0.2248, grad=0.0000), Var(v=0.0461, grad=0.0000), Var(v=-0.0302, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0450, grad=0.0000), Var(v=-0.0624, grad=0.0000), Var(v=0.0849, grad=0.0000), Var(v=0.0424, grad=0.0000), Var(v=0.0392, grad=0.0000), Var(v=0.0031, grad=0.0000)], [Var(v=0.0584, grad=0.0000), Var(v=0.0917, grad=0.0000), Var(v=0.0476, grad=0.0000), Var(v=0.1390, grad=0.0000), Var(v=-0.0154, grad=0.0000), Var(v=-0.1351, grad=0.0000), Var(v=0.1509, grad=0.0000), Var(v=0.1654, grad=0.0000), Var(v=-0.1658, grad=0.0000), Var(v=-0.2197, grad=0.0000), Var(v=-0.0155, grad=0.0000), Var(v=-0.1135, grad=0.0000), Var(v=-0.1935, grad=0.0000), Var(v=0.0483, grad=0.0000), Var(v=-0.0971, grad=0.0000), Var(v=-0.1751, grad=0.0000), Var(v=-0.0424, grad=0.0000), Var(v=-0.0428, grad=0.0000), Var(v=0.2091, grad=0.0000), Var(v=-0.1110, grad=0.0000), Var(v=0.0075, grad=0.0000), Var(v=0.0712, grad=0.0000), Var(v=0.1351, grad=0.0000), Var(v=-0.0624, grad=0.0000), Var(v=0.1825, grad=0.0000), Var(v=-0.0304, grad=0.0000), Var(v=0.0346, grad=0.0000), Var(v=-0.1420, grad=0.0000), Var(v=-0.0140, grad=0.0000), Var(v=0.0124, grad=0.0000), Var(v=-0.0159, grad=0.0000), Var(v=-0.1047, grad=0.0000), Var(v=0.0906, grad=0.0000), Var(v=0.0362, grad=0.0000), Var(v=-0.0606, grad=0.0000), Var(v=-0.0730, grad=0.0000), Var(v=0.1020, grad=0.0000), Var(v=-0.0258, grad=0.0000), Var(v=-0.0365, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=-0.2374, grad=0.0000), Var(v=0.0131, grad=0.0000), Var(v=-0.0267, grad=0.0000), Var(v=-0.1470, grad=0.0000), Var(v=-0.1482, grad=0.0000), Var(v=0.0182, grad=0.0000), Var(v=-0.1120, grad=0.0000), Var(v=0.1016, grad=0.0000), Var(v=0.0931, grad=0.0000), Var(v=0.1261, grad=0.0000)], [Var(v=-0.1426, grad=0.0000), Var(v=-0.0933, grad=0.0000), Var(v=0.1264, grad=0.0000), Var(v=-0.0269, grad=0.0000), Var(v=0.1397, grad=0.0000), Var(v=0.0151, grad=0.0000), Var(v=0.0394, grad=0.0000), Var(v=0.0900, grad=0.0000), Var(v=0.0323, grad=0.0000), Var(v=0.1841, grad=0.0000), Var(v=0.2547, grad=0.0000), Var(v=-0.2196, grad=0.0000), Var(v=0.1067, grad=0.0000), Var(v=-0.2187, grad=0.0000), Var(v=-0.0582, grad=0.0000), Var(v=0.2172, grad=0.0000), Var(v=0.0281, grad=0.0000), Var(v=-0.1337, grad=0.0000), Var(v=0.0508, grad=0.0000), Var(v=0.0743, grad=0.0000), Var(v=0.1033, grad=0.0000), Var(v=0.0939, grad=0.0000), Var(v=0.1118, grad=0.0000), Var(v=-0.0352, grad=0.0000), Var(v=0.0066, grad=0.0000), Var(v=0.0766, grad=0.0000), Var(v=-0.1084, grad=0.0000), Var(v=-0.1309, grad=0.0000), Var(v=-0.1987, grad=0.0000), Var(v=-0.2852, grad=0.0000), Var(v=-0.0485, grad=0.0000), Var(v=0.2989, grad=0.0000), Var(v=0.0259, grad=0.0000), Var(v=0.0190, grad=0.0000), Var(v=0.0646, grad=0.0000), Var(v=-0.0676, grad=0.0000), Var(v=0.0175, grad=0.0000), Var(v=0.0146, grad=0.0000), Var(v=0.0524, grad=0.0000), Var(v=-0.0307, grad=0.0000), Var(v=0.0720, grad=0.0000), Var(v=0.0673, grad=0.0000), Var(v=0.0497, grad=0.0000), Var(v=-0.0441, grad=0.0000), Var(v=0.0238, grad=0.0000), Var(v=0.0905, grad=0.0000), Var(v=0.0167, grad=0.0000), Var(v=-0.1544, grad=0.0000), Var(v=0.0028, grad=0.0000), Var(v=-0.0946, grad=0.0000)], [Var(v=-0.0753, grad=0.0000), Var(v=-0.1132, grad=0.0000), Var(v=0.0495, grad=0.0000), Var(v=0.0875, grad=0.0000), Var(v=-0.0670, grad=0.0000), Var(v=0.0840, grad=0.0000), Var(v=-0.0314, grad=0.0000), Var(v=-0.0944, grad=0.0000), Var(v=-0.1870, grad=0.0000), Var(v=0.0411, grad=0.0000), Var(v=-0.0795, grad=0.0000), Var(v=0.1164, grad=0.0000), Var(v=0.0526, grad=0.0000), Var(v=-0.0680, grad=0.0000), Var(v=-0.0784, grad=0.0000), Var(v=-0.0779, grad=0.0000), Var(v=0.1171, grad=0.0000), Var(v=-0.1213, grad=0.0000), Var(v=0.0811, grad=0.0000), Var(v=-0.1726, grad=0.0000), Var(v=-0.1346, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=0.0619, grad=0.0000), Var(v=0.1117, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=-0.0416, grad=0.0000), Var(v=-0.0690, grad=0.0000), Var(v=0.0169, grad=0.0000), Var(v=-0.0663, grad=0.0000), Var(v=0.1403, grad=0.0000), Var(v=-0.1084, grad=0.0000), Var(v=0.0308, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=0.0593, grad=0.0000), Var(v=0.1364, grad=0.0000), Var(v=-0.1611, grad=0.0000), Var(v=-0.0232, grad=0.0000), Var(v=0.0045, grad=0.0000), Var(v=0.0505, grad=0.0000), Var(v=-0.0200, grad=0.0000), Var(v=-0.0070, grad=0.0000), Var(v=-0.1070, grad=0.0000), Var(v=0.0643, grad=0.0000), Var(v=-0.0279, grad=0.0000), Var(v=0.0343, grad=0.0000), Var(v=0.0972, grad=0.0000), Var(v=0.0833, grad=0.0000), Var(v=0.0465, grad=0.0000), Var(v=-0.0727, grad=0.0000), Var(v=-0.1025, grad=0.0000)]] Biases: [Var(v=0.0488, grad=0.0000), Var(v=-0.0900, grad=0.0000), Var(v=-0.0581, grad=0.0000), Var(v=-0.0061, grad=0.0000), Var(v=-0.0363, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0370, grad=0.0000), Var(v=-0.0024, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0446, grad=0.0000), Var(v=0.0002, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.1110, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0817, grad=0.0000), Var(v=0.0162, grad=0.0000), Var(v=0.0019, grad=0.0000), Var(v=-0.0214, grad=0.0000), Var(v=0.2356, grad=0.0000), Var(v=0.0378, grad=0.0000), Var(v=0.0032, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0556, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.1176, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0428, grad=0.0000), Var(v=-0.0691, grad=0.0000), Var(v=-0.0048, grad=0.0000), Var(v=-0.1273, grad=0.0000), Var(v=-0.0070, grad=0.0000), Var(v=0.1384, grad=0.0000), Var(v=0.0011, grad=0.0000), Var(v=-0.0006, grad=0.0000), Var(v=0.0238, grad=0.0000), Var(v=-0.0032, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.1182, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0131, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0256, grad=0.0000), Var(v=-0.0273, grad=0.0000), Var(v=-0.0629, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.1209, grad=0.0000), Var(v=0.0066, grad=0.0000), Var(v=0.0808, grad=0.0000), Var(v=-0.0024, grad=0.0000)]\n",
            "Layer 2 \n",
            " Weights: [[Var(v=-0.0839, grad=0.0000)], [Var(v=-0.1018, grad=0.0000)], [Var(v=0.0470, grad=0.0000)], [Var(v=-0.2679, grad=0.0000)], [Var(v=-0.0171, grad=0.0000)], [Var(v=-0.1486, grad=0.0000)], [Var(v=0.0107, grad=0.0000)], [Var(v=-0.1330, grad=0.0000)], [Var(v=-0.0833, grad=0.0000)], [Var(v=0.0331, grad=0.0000)], [Var(v=0.0271, grad=0.0000)], [Var(v=0.0255, grad=0.0000)], [Var(v=0.1540, grad=0.0000)], [Var(v=-0.2298, grad=0.0000)], [Var(v=-0.1519, grad=0.0000)], [Var(v=0.1093, grad=0.0000)], [Var(v=0.0282, grad=0.0000)], [Var(v=0.0809, grad=0.0000)], [Var(v=-0.0111, grad=0.0000)], [Var(v=0.2852, grad=0.0000)], [Var(v=0.0496, grad=0.0000)], [Var(v=0.1493, grad=0.0000)], [Var(v=-0.0069, grad=0.0000)], [Var(v=0.0650, grad=0.0000)], [Var(v=-0.0502, grad=0.0000)], [Var(v=-0.1017, grad=0.0000)], [Var(v=0.0748, grad=0.0000)], [Var(v=-0.0394, grad=0.0000)], [Var(v=-0.0668, grad=0.0000)], [Var(v=-0.2182, grad=0.0000)], [Var(v=0.1453, grad=0.0000)], [Var(v=0.0011, grad=0.0000)], [Var(v=0.1734, grad=0.0000)], [Var(v=0.0568, grad=0.0000)], [Var(v=0.0181, grad=0.0000)], [Var(v=-0.0348, grad=0.0000)], [Var(v=-0.1454, grad=0.0000)], [Var(v=-0.0316, grad=0.0000)], [Var(v=-0.1235, grad=0.0000)], [Var(v=-0.0368, grad=0.0000)], [Var(v=0.0010, grad=0.0000)], [Var(v=0.1221, grad=0.0000)], [Var(v=0.0096, grad=0.0000)], [Var(v=-0.0124, grad=0.0000)], [Var(v=-0.0331, grad=0.0000)], [Var(v=0.1181, grad=0.0000)], [Var(v=-0.1624, grad=0.0000)], [Var(v=-0.0080, grad=0.0000)], [Var(v=-0.1281, grad=0.0000)], [Var(v=-0.1041, grad=0.0000)]] Biases: [Var(v=0.0232, grad=0.0000)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ],
      "source": [
        "print('Network before update:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
        "\n",
        "def parameters(network):\n",
        "  params = []\n",
        "  for layer in range(len(network)):\n",
        "    params += network[layer].parameters()\n",
        "  return params\n",
        "\n",
        "def update_parameters(params, learning_rate=0.01):\n",
        "  for p in params:\n",
        "    p.v -= learning_rate*p.grad\n",
        "\n",
        "def zero_gradients(params):\n",
        "  for p in params:\n",
        "    p.grad = 0.0\n",
        "\n",
        "update_parameters(parameters(NN))\n",
        "\n",
        "print('\\nNetwork after update:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
        "\n",
        "zero_gradients(parameters(NN))\n",
        "\n",
        "print('\\nNetwork after zeroing gradients:')\n",
        "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "woWYpdw6FtIO"
      },
      "outputs": [],
      "source": [
        "# Initialize an arbitrary neural network\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "# Recommended hyper-parameters for 3-D: \n",
        "#NN = [\n",
        "#    DenseLayer(3, 16, lambda x: x.relu()),\n",
        "#    DenseLayer(16, 1, lambda x: x.identity())\n",
        "#]\n",
        "\n",
        "\n",
        "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
        "## of the activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "mdqaqYBVFtIR"
      },
      "outputs": [],
      "source": [
        "# Initialize training hyperparameters\n",
        "EPOCHS = 200\n",
        "LEARN_R = 2e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "5kfg76GMFtIW",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf531f0b-3bfe-4c5d-b15a-552bd379ec1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0 ( 0.00%) Train loss: 105.435 \t Validation loss: 102.896\n",
            "  10 ( 5.00%) Train loss: 24.531 \t Validation loss: 17.409\n",
            "  20 (10.00%) Train loss: 14.909 \t Validation loss: 11.024\n",
            "  30 (15.00%) Train loss: 14.476 \t Validation loss: 10.732\n",
            "  40 (20.00%) Train loss: 14.166 \t Validation loss: 10.582\n",
            "  50 (25.00%) Train loss: 13.806 \t Validation loss: 10.409\n",
            "  60 (30.00%) Train loss: 13.332 \t Validation loss: 10.138\n",
            "  70 (35.00%) Train loss: 12.815 \t Validation loss: 9.818\n",
            "  80 (40.00%) Train loss: 12.448 \t Validation loss: 9.599\n",
            "  90 (45.00%) Train loss: 12.215 \t Validation loss: 9.493\n",
            " 100 (50.00%) Train loss: 12.085 \t Validation loss: 9.398\n",
            " 110 (55.00%) Train loss: 12.002 \t Validation loss: 9.293\n",
            " 120 (60.00%) Train loss: 11.942 \t Validation loss: 9.193\n",
            " 130 (65.00%) Train loss: 11.917 \t Validation loss: 9.145\n",
            " 140 (70.00%) Train loss: 11.905 \t Validation loss: 9.114\n",
            " 150 (75.00%) Train loss: 11.899 \t Validation loss: 9.086\n",
            " 160 (80.00%) Train loss: 11.895 \t Validation loss: 9.066\n",
            " 170 (85.00%) Train loss: 11.893 \t Validation loss: 9.052\n",
            " 180 (90.00%) Train loss: 11.893 \t Validation loss: 9.048\n",
            " 190 (95.00%) Train loss: 11.893 \t Validation loss: 9.044\n"
          ]
        }
      ],
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "    \n",
        "    if e%10==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "VetyRWFwFtIY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e6b3fd6a-37b8-410d-d315-584c182a7314"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OgmIrM9FtIb"
      },
      "source": [
        "# Testing\n",
        "\n",
        "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "HmNi7S-vFtIc"
      },
      "outputs": [],
      "source": [
        "output_test = forward(x_test, NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "7mmJOTSEFtIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "3929eb9a-998e-440a-b517-b83bd27686da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss:  9.857\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "y_test_np = Var_to_nparray(y_test)\n",
        "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
        "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
        "plt.xlabel(\"y\");\n",
        "plt.ylabel(\"$\\hat{y}$\");\n",
        "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
        "plt.grid(True);\n",
        "plt.axis('equal');\n",
        "plt.tight_layout();\n",
        "\n",
        "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
        "\n",
        "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "ODi0WlmQFtIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "98eef8c4-7c92-40f8-e46d-750246e2e905"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x_test_np = Var_to_nparray(x_test)\n",
        "x_train_np = Var_to_nparray(x_train)\n",
        "y_train_np = Var_to_nparray(y_train)\n",
        "if D1:\n",
        "    plt.scatter(x_train_np, y_train_np, label=\"train data\");\n",
        "    plt.scatter(x_test_np, Var_to_nparray(output_test), label=\"test prediction\");\n",
        "    plt.scatter(x_test_np, y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "else:\n",
        "    plt.scatter(x_train_np[:,1], y_train, label=\"train data\");\n",
        "    plt.scatter(x_test_np[:,1], Var_to_nparray(output_test), label=\"test data prediction\");\n",
        "    plt.scatter(x_test_np[:,1], y_test_np, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBAmjsAFtIk"
      },
      "source": [
        "\n",
        "## Exercise k) Show overfitting, underfitting and just right fitting\n",
        "\n",
        "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
        "\n",
        "See also if you can get a good compromise which leads to a low validation loss. \n",
        "\n",
        "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
        "\n",
        "_Insert written answer here._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "tQZCn2dxFtIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "bb7eb129-982a-46aa-9218-c94159e55ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0 ( 0.00%) Train loss: 10.229 \t Validation loss: 109.210\n",
            "  10 ( 5.00%) Train loss: 9.836 \t Validation loss: 104.723\n",
            "  20 (10.00%) Train loss: 9.546 \t Validation loss: 101.278\n",
            "  30 (15.00%) Train loss: 9.244 \t Validation loss: 97.698\n",
            "  40 (20.00%) Train loss: 8.868 \t Validation loss: 93.318\n",
            "  50 (25.00%) Train loss: 8.363 \t Validation loss: 87.549\n",
            "  60 (30.00%) Train loss: 7.681 \t Validation loss: 79.951\n",
            "  70 (35.00%) Train loss: 6.799 \t Validation loss: 70.365\n",
            "  80 (40.00%) Train loss: 5.734 \t Validation loss: 59.092\n",
            "  90 (45.00%) Train loss: 4.575 \t Validation loss: 47.131\n",
            " 100 (50.00%) Train loss: 3.465 \t Validation loss: 35.941\n",
            " 110 (55.00%) Train loss: 2.554 \t Validation loss: 26.891\n",
            " 120 (60.00%) Train loss: 1.915 \t Validation loss: 20.597\n",
            " 130 (65.00%) Train loss: 1.528 \t Validation loss: 16.767\n",
            " 140 (70.00%) Train loss: 1.319 \t Validation loss: 14.667\n",
            " 150 (75.00%) Train loss: 1.209 \t Validation loss: 13.605\n",
            " 160 (80.00%) Train loss: 1.149 \t Validation loss: 13.116\n",
            " 170 (85.00%) Train loss: 1.112 \t Validation loss: 12.926\n",
            " 180 (90.00%) Train loss: 1.086 \t Validation loss: 12.895\n",
            " 190 (95.00%) Train loss: 1.065 \t Validation loss: 12.944\n",
            "Another option would be infinitly high EPOCHS, but that takes time\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xU9b3/8ddnZrazsOxSpIOK3hisWQ32XCsaI0TUSCwkthhLNCa/XHNzE5ObcmOKMcVeEjRENKCBGKMR1NiJKxZQQIgCgpSlLmwv398f37PLsuxSdnbmTHk/H495zDnfObPz4czwnpnvnPP9mnMOERHJLJGwCxARkZ6ncBcRyUAKdxGRDKRwFxHJQAp3EZEMFAu7AIB+/fq5kSNHhl2GiEhaeeONN9Y75/p3dltKhPvIkSOpqKgIuwwRkbRiZsu7uk3dMiIiGUjhLiKSgRTuIiIZSOEuIpKBFO4iIhlI4S4ikoEU7iIiGSi9w73yfXj2R9BYF3YlIiIpJb3D/f2/wws/h7uOhWUvhV2NiEjKSO9wP/Z6uPhxaG6EP3wWZl0H1evDrkpEJHTpHe4A+50EV78Gx3wN3pwKvzkcXrpNXTUiktXSP9wBcgvhtB/6kB9+NMy+GW4/Ct59HDSNoIhkocwI91b9D4ALH/VdNbm94M9fgntPgvf/oZAXkaySWeHear+T4KoX4ezfQs16+NN5cO9/wuK/K+RFJCtkZrgDRKJwxCVw3bwg5DfCwxfAPSfCghnQ3BR2hSIiCZO54d4qmhOE/Bsw/g5oqIbpl8JvDoOXfwO1m8OuUESkx2V+uLeK5sDhF8I1r8OkadB3JDzzXfjVJ+HvN8HGD8OuUESkx6TETExJFYnAgWf4y+q34dU74PV74V93w+jTofxS2P9k360jIpKmzKXAD4zl5eUu1Gn2qj6G1++HeQ9C9TooGQ6f+jIcfjH06nR6QhGR0JnZG8658k5vU7i309QAi56Aigdg2YsQyYGDzvaf5kccC2ZhVygi0mZX4Z593TK7EsuFMef4S+X78Mbv4a2p/uiafgf6kD/0AigoCbtSEZFd0if33Wmo8We6VjwAqyogVgAHT4Tyy2DIEWFXJyJZbFef3Hd7tIyZPWBm68xsQbu2UjN7xsyWBNd9g3Yzs9+Y2VIze8fM0j/9cgv9UTZXzIGvvACHnA8LHvMnRd19ou+nb6gOu0oRkR3syaGQfwDGdWi7CZjjnBsNzAnWAc4ARgeXK4E7e6bMFDHoUDj7N/CNRXDmL6Cp3o9E+ctPwJPfgnWLwq5QRATYg3B3zr0AbOzQPB6YEixPASa0a3/Qea8BJWY2qKeKTRn5feCoK+DqV+HLT8EBp/n++Ts+Db8/E+ZP98EvIhKS7p7ENNA5tzpYXgMMDJaHAB+1225l0LYTM7vSzCrMrKKysrKbZYTMDEYcDRPvgxsXwik/gKpVMOMyf3LUcz+BbevCrlJEslDcZ6g6/4vsXv8q65y7xzlX7pwr798/A44lL+oHx90A170JF82AIZ+Cf97iQ37mNbD2vbArFJEs0t1DIdea2SDn3Oqg26X14+kqYFi77YYGbdkjEoH9T/GX9Uth7p1+EpE3/wj7/iccfQ3sd7LfTkQkQbqbMLOAycHyZGBmu/ZLgqNmxgJb2nXfZJ9++8Nnfwk3vgcnfw8qF8HUc+GOsfDGFPXLi0jC7MmhkA8DrwIHmtlKM7sM+ClwqpktAU4J1gGeBD4AlgL3AlcnpOp0U1gKx38Drn8HPn8PxPLgr1/zUwLOvRsaa8OuUEQyjE5iCoNz8O9n4YWfw4pXoWgAHHOtPwM2rzjs6kQkTcR1EpMkgJkfefLSp+BLT8LAT8Iz34PbDoZ//hzqt4ZdoYikOYV72EYeC5f8BS6fA8M+Dc/9CH59KLx6OzTWhV2diKQphXuqGFoOX3wErngW9jkYnv5v+O2n/PAGmhJQRPaSwj3VDPkUXDITLpkFxfv44Q3uPAaWzg67MhFJIwr3VLXviXD5bPjCVGhphD9OhKnn+aGIRUR2Q+GeyszgE2fB1a/BaT+CFa/BnUf7OV/rtoRdnYikMIV7OojlwTHXwXXz/NR//7obfnekn0QkBQ5lFZHUo3BPJ736w+du80fWFA+C6Zf6M143fhh2ZSKSYhTu6WjIEf6omnG3wIq5fjiDF2/VUTUi0kbhnq4iURh7FVz7Lxh9Ksz5ATxwmn5wFRFA4Z7+eg+GL/wRzn0ANn4Adx/vT4BqaQm7MhEJkcI9U4yZCFfPhX0/40+A+sNnYdPysKsSkZAo3DNJ8UCYNA3G3wFrF8Bdx8N7M3d/PxHJOAr3TGMGh18IX3kByvaDRy+BJ27UODUiWUbhnqlKR8GlT8PR10LF/XDfKbB+SdhViUiSKNwzWSwXTv8xfPFRP3H33SfCe7PCrkpEkkDhng0OOB2uegkG/Ac8ejE8+yMdTSOS4RTu2aLPED8xyGEX+Rmgpk3S+DQiGUzhnk1y8mH87+DMX/ghhO89SSc9iWQohXu2MYOjrvDjxdduhvtPgWUvh12ViPQwhXu2GnksXDHHT8790ASYPz3sikSkByncs1nfkXDZP2DokTDjMj/4mIYQFskICvdsV1gKFz8OY871g489cYNGlxTJALGwC5AUEMuDc+6FkuHw0q1QuwnOuc8fJy8iaUnhLl4kAqfcDEX94elvQ0M1nP8Q5BaGXZmIdIO6ZWRHR18NZ/8Wls7xk3LrWHiRtKRwl50dcQmcez+s/BdMORuqN4RdkYjspbjC3cy+bmbvmtkCM3vYzPLNbJSZzTWzpWb2iJmp4zYdjZkIF/wJKhfBlLOgen3YFYnIXuh2uJvZEOBrQLlzbgwQBS4AbgF+5ZzbH9gEXNYThUoIDjjdDzq28UN4cLw+wYukkXi7ZWJAgZnFgEJgNXAS0HpGzBRgQpyPIWHa90SY9DBsWAoPjYeajWFXJCJ7oNvh7pxbBfwCWIEP9S3AG8Bm51zrgdIrgSHxFikh2+8/4YKpfhyahyb4QyVFJKXF0y3TFxgPjAIGA0XAuL24/5VmVmFmFZWVld0tQ5Jl/1P8RNzrFsJD5+goGpEUF0+3zCnAh865SudcI/AYcCxQEnTTAAwFVnV2Z+fcPc65cudcef/+/eMoQ5LmgNP8se9r5sPDk6CxNuyKRKQL8YT7CmCsmRWamQEnA+8BzwHnBttMBjRDcyY5cBycczcsfwWmX6qhCkRSVDx97nPxP5zOA+YHf+se4L+AG81sKVAG3N8DdUoqGTMRzvw5LH4SZl2nWZ1EUlBcww84524Gbu7Q/AFwVDx/V9LAUVf4I2ee/4kffOy0H/mx4kUkJWhsGem+E78FNevh1d9BYRkcf2PYFYlIQOEu3WcG427xn+Dn/AD6DINDzgu7KhFB4S7xikRgwh2wdTXMvBp6D/azPIlIqDRwmMQvluePgS8ZAdO+COuXhF2RSNZTuEvPKCyFCx+FSBSmnquBxkRCpnCXnlO6L0yaBlvX6CQnkZAp3KVnDTsKPn+3Hwt+5rWacFskJAp36XmfnAAnfRcWTIeXbwu7GpGspHCXxDj+G/DJc2D2D+D9p8OuRiTrKNwlMcxg/O2wz8Ew43I/XLCIJI3CXRInt9BP1RfNhWmToHZz2BWJZA2FuyRWyTD4wkOwaTnMuAxamsOuSCQrKNwl8UYc40eRXDobnv9p2NWIZAWFuyRH+ZfhsIvghZ/BkmfCrkYk4yncJXk++wsYeDA8dgVsXhF2NSIZTeEuyZNTAOdP8f3uj14CTfVhVySSsRTuklxl+8GEO+HjN+Gpm8KuRiRjKdwl+T5xFhzzNah4AN5+JOxqRDKSwl3CcfLNMOJY+Ov1sPa9sKsRyTgKdwlHNAbnPgD5veHRi6GuKuyKRDKKwl3CU7wPnPt72PgB/O0bGkFSpAcp3CVcI4+FE2+C+Y/C2w+HXY1IxlC4S/hO+CaMOA7+9k1YvzTsakQygsJdwheJwjn3+LlYp39Jx7+L9ACFu6SGPkNgwh2wZj48872wqxFJewp3SR0HngGfvgrm3gWL/x52NSJpTeEuqeXU//UTfPzlaqj6OOxqRNKWwl1SSyzPHx7ZVA8zrtD47yLdFFe4m1mJmU03s0VmttDMjjazUjN7xsyWBNd9e6pYyRL9RvsRJJe/BC/dGnY1Imkp3k/uvwaecs79B3AosBC4CZjjnBsNzAnWRfbOoZNgzEQ/uceqeWFXI5J2uh3uZtYHOAG4H8A51+Cc2wyMB6YEm00BJsRbpGQhM/jsL6HXQD/+e0N12BWJpJV4PrmPAiqB35vZm2Z2n5kVAQOdc6uDbdYAAzu7s5ldaWYVZlZRWVkZRxmSsQr6wufvgg3/hn/8T9jViKSVeMI9BhwB3OmcOxyopkMXjHPOAZ0OGOKcu8c5V+6cK+/fv38cZUhGG3UCHHOdHx548VNhVyOSNuIJ95XASufc3GB9Oj7s15rZIIDgel18JUrWO+l//PR8M6+BbXo5ieyJboe7c24N8JGZHRg0nQy8B8wCJgdtk4GZcVUoEsuDifdC/VaYea1GjxTZA/EeLXMdMNXM3gEOA34C/BQ41cyWAKcE6yLxGfAJf4LTkqd9F42I7FIsnjs7594Cyju56eR4/q5Ip466Epb8A57+ju+L7zc67IpEUpbOUJX0EYnA+NshpwBmXA5NDWFXJJKyFO6SXnoPgs/9Gla/Bf9Uj59IVxTukn4OOhsOvwhevBWWvxJ2NSIpSeEu6WncLdB3JDz2FajbEnY1IilH4S7pKa8XnHMvVK2CJ78VdjUiKUfhLulr2JFwwv+Dd6bBgsfCrkYkpSjcJb2d8E0YUg5PfB22rAq7GpGUoXCX9BbN8ZNrNzfCX74KLS1hVySSEhTukv7K9oNxP4EP/wlz7wy7GpGUoHCXzHDEZDjwTJj9fVj7btjViIRO4S6ZwQzO/i3kl/i5Vxvrwq5IJFQKd8kcRf388ATr3oVnfxh2NSKhUrhLZjngNDjycnj1d/DB82FXIxIahbtknlN/CGWj4fGvQu2msKsRCYXCXTJPbqGf3KN6nT/+XZN7SBZSuEtmGnw4fObb8O7j8M6jYVcjknQKd8lcx30dho2FJ78Jm5aHXY1IUincJXNFonDO3b5b5vGroKU57IpEkkbhLpmt70g48+ew4hV4+ddhVyOSNAp3yXyHXgAHTYDnfgwfvxV2NSJJoXCXzGcGZ/0KivrDY1dAQ03YFYkknMJdskNhKUy4A9a/D898N+xqRBJO4S7ZY7+TYOw18Pp9sPCJsKsRSSiFu2SXU26GQYfBzGtg80dhVyOSMAp3yS6xPDj3AWhpghmXQ3NT2BWJJITCXbJP2X5w1m3w0Wvw/P+FXY1IQijcJTsdch4cdhG8+EuNHikZSeEu2evMn0HZ/vDYlbCtMuxqRHpU3OFuZlEze9PMngjWR5nZXDNbamaPmFlu/GWKJEBuEZz3B6jdDH+5SpNrS0bpiU/u1wML263fAvzKObc/sAm4rAceQyQx9hnjJ9deOhte/W3Y1Yj0mLjC3cyGAp8F7gvWDTgJmB5sMgWYEM9jiCRc+WXwic/B7B/A8lfCrkakR8T7yf024FtA6/fZMmCzc671+LKVwJDO7mhmV5pZhZlVVFaqv1NCZObnXu07Av78Zdi6NuyKROLW7XA3s7OAdc65N7pzf+fcPc65cudcef/+/btbhkjPyO8D5z8EdVtg+qU6/l3SXjyf3I8FzjazZcA0fHfMr4ESM4sF2wwFVsVVoUiy7DPGDzC2/CV49n/DrkYkLt0Od+fct51zQ51zI4ELgGedcxcCzwHnBptNBmbGXaVIshw2Ccov9WO/L/xr2NWIdFsijnP/L+BGM1uK74O/PwGPIZI4437q52D9y9Ww4d9hVyPSLT0S7s65551zZwXLHzjnjnLO7e+cO885V98TjyGSNLE8OP9BiMTg4Um+H14kzegMVZHOlAyH86fAxn/DjCs0/6qkHYW7SFdGnQBn3AJLnoY5+oFV0kts95uIZLEjL4e178LLt8GAg+DQL4Rdkcge0Sd3kd0542cw4jiYdR2s7NZpHSJJp3AX2Z1ojv+BtXggTPuiZnCStKBwF9kTRWUw6RForIGp50LtprArEtklhbvInhp4EFww1R/7Pu0iaNJRvpK6FO4ie2PUCTDhTj9EweMaA15Sl46WEdlbh5wHVatg9s3QZwic9qOwKxLZicJdpDuOvd4H/Cu/heLBcPTVYVcksgOFu0h3mPkxaLaugae/DTkFUP7lsKsSaaM+d5HuikRh4v0w+jR44uvw9rSwKxJpo3AXiUcs10/yMeoE+MtXYcFjYVckAijcReKXkw+THoZhY2HG5bDob2FXJKJwF+kRuUXwxUf8OPCPToZFT4ZdkWQ5hbtIT8nvDRdNh0GHwCMXwfzpYVckWUzhLtKTCvrCJTNheNBFM+/BsCuSLKVwF+lpecVw4XTY/2Q/kuRLt4FzYVclWUbhLpIIuYVwwZ9gzER/Juvfv6XZnCSpdBKTSKLE8uCc+6D3YH8ma9XHcM69PvhFEkyf3EUSKRLxY8+Mu8UfIvn7M2DLqrCrkiygcBdJhrFXwaRpfrjgez4DK+aGXZFkOIW7SLIcOA4unw15vWDKWfD6/fqhVRJG4S6STAP+A654FkadCH+7EaZfCnVVYVclGUjhLpJsBX3hi4/CyTfDezPhnhNh1bywq5IMo3AXCUMkAsffCF96Ahrr4L5T4PmfQnNj2JVJhlC4i4RpxDFw9Sv+ePjn/w/uPxXWLQy7KskA3Q53MxtmZs+Z2Xtm9q6ZXR+0l5rZM2a2JLju23PlimSggr4w8V44bwpsWg53HQezfwANNWFXJmksnk/uTcA3nHMHAWOBa8zsIOAmYI5zbjQwJ1gXkd355AS49nU4+Hx46Va4YywsfkpH1Ei3dDvcnXOrnXPzguWtwEJgCDAemBJsNgWYEG+RIlmjqB98/k6Y/FeI5sLDX4AHx8Oa+WFXJmmmR/rczWwkcDgwFxjonFsd3LQGGNjFfa40swozq6isrOyJMkQyx6gT4Kuv+DNb17wDdx0Pj18F65eGXZmkCXNxfuUzs17AP4EfO+ceM7PNzrmSdrdvcs7tst+9vLzcVVRUxFWHSMaq3QQv/MKf9NRcD2POhRO+Cf0PDLsyCZmZveGcK+/strg+uZtZDjADmOqca508cq2ZDQpuHwSsi+cxRLJeQV84/cdwwztw9LWw6Am4/dPw5y/Dygr1yUun4jlaxoD7gYXOuVvb3TQLmBwsTwZmdr88EWnTawCc9kO4YT4cdwMseQbuO9mPVfPmH6GxNuwKJYV0u1vGzI4DXgTmAy1B83/j+90fBYYDy4HznXMbd/W31C0j0g31W+HtafD6fVC5CPJL4NBJcMh5MPgIMAu7QkmwXXXLxN3n3hMU7iJxcA6Wvwz/uhcWPwnNDVC6r++bP/g86H9A2BVKgijcRbJF7WZY+FeY/2dY9iK4FigbDQecDqNPheHHQCw37CqlhyjcRbLR1rV+YLL3n4JlL/kjbXJ7+cMsRxwLI46GfQ6FqCZkS1e7Cnc9qyKZqnggfPpKf2mohg9fgPefhg+e89034MN+6JH+MuhQGHwY9B6i/voMoHAXyQa5RXDgGf4CULUaVrwCy1+FFa/Ci7/wXTgAhWWwzyH+OPp+o323Tr8DoHgfhX4aUbiLZKPeg/xIlGMm+vWGGlj7Lqx+C1a/7Yc7mPcQNFZvv09uMZTtByXDoc8w6DME+gz1l95D/dAJkWg4/x7ZicJdRCC3EIYd6S+tnIOtq2H9+7B+ib9sWAqVi2HpbGjsOGqlQWEpFPbzQV/Ub/tyYRnk9Ya84naXYD2/N8Ty9a2ghyncRaRzZtB7sL/s+5kdb3POD4uwZSVUrfLX1ZXBZT3UbIB1i/x67SZgdwdumA/4WJ6/zsnfcT3Wbj0SCy5RsKi/jkR9W/t1a23bzZvGbg8qcb7LyrVAS3Ow7MA1d2hrXW/Zvv1O23T8O83w6a/6+XV7mMJdRPaetX5KL4VBh+x62+YmqNsC9VX+xKsdLlv8HLJNdf4M26Z6v9x2qd/eXrfFt7U0+XBsafbh2NLs29qW26837ek/aDc3R4I3jIh/07BI0BbZsS3S7ra2bTrcr+N9WhIz+5bCXUQSKxqDojJ/kaTRNHsiIhlI4S4ikoHSulvm+cXreGrBGoryYv6SG6UoL0av1vW8KL3yYhTm+raC3CgFOVFyoobpl3kRyWBpHe4fbaplzqJ11NQ3Ud3QvMf3ixjk50T9JRYhPydKXk6UgpzI9vacCPmx1vZgvf11LEp+8GbR/vbWN5D8oD0vFiES0RuJiCRXWof7xWNHcPHYEQC0tDhqGpupqW9iW30T1fXNbKtvoqZh+3pNQxP1TS3UNTZT19hMbWMzdY2t6y3UNzVT29BMVV3jDu2t2ze1dG8cnvycyA6B3/om0HV7tNNvIcV5OW3fRoryYhTmRvUNREQ6ldbh3l4kYvQKwnBAgh6jqbmFuuDNobahOXgzaKEueFOoDd4EWm+vbdzxjaS2oZm6phZ/HbRvrmn092t3W23jnn0LMYNeuTt2QRXlxSjOj1FSkEufwhz6FGy/lATrJQW59CnIoTg/pm8VIhkqY8I9GWLRCL2iEXrlJXa3OeeobWxmW92O30L8ctNOy9vbmqmub6KyspottZvZUttIfVNLl49jBr3zfeiXFeVS1iuPfr3y6Ncrt229rFcu/XrlUVaUS0lhLlG9GYikBYV7CjIzCnP9D8Hxfgupa2xmS20jW2ob2VzTet3AltpGqmob2VzbyKaaRjZW1/PRxhreXLGZjdX1dNYDFTEoLcqlrCiPAb3z2Kd3Pvv0yWdg7/wdlsuKcvWNQCRkCvcM1/oD8cDe+Xt8n+YWx+aaBjZUN7B+Wz0btjWwYVt9sO7b1lXVsXjNVtZv2/mNICdqDCjOZ2DvvLbAH1JSwNC+BQwpKWRo3wJKCnP0e4FIAincZSfRiAVdMnkcMLB4l9s2NbdQua2eNVvqWFtVx9qqetZU1bF2Sx1rqupYtGYrzy+upKbD0UyFuVGG9i1gaN/C7cHfbr1fr1yFv0gcFO4Sl1g0wqA+BQzqU9DlNs45ttQ2snJTLSs31bJqcy0rN9X45U21VCzbSFXdjmOAFOREGV5ayPCyQkaWFTK8rIgRpYWMKPPhH4vq/DuRXVG4S8KZGSWF/gfZMUP6dLpNVV0jq4LwX7mpho821rJiYzXL1lfzwvuVO/wwHI0YQ0oKGFHmw35EaRHDg+XhpYUU5uplLaL/BZISeufn0HtQDp8Y1Hun21paHOu21rNsQzUrNtSwfGM1yzfUsGJjDbPe+ninT/39i/OCT/lFbW8Aw0v9pbRI3T2SHRTukvIiEWOfPv5onLH77jyy4OaaBpZvqGH5xhpWbKhuW35paSUz5tXvsG1RbpRhpdvDfnhZYdv6kJIC8nM0k5BkBoW7pL3WLp9Dh5XsdFttQzMfbaphRfBJf8XGGj7aWMOyDdW8sKSSusbt3T1mMLA4n+Gl2wN/eFkBw/oWMqikgIHFeerrl7ShcJeMVpAb5YCBxZ0e9eOco3KbP75/xcYaVmyobQv/l5euZ0ZV3Q7bRwwGFOczqCSfwX0KGNQnn8ElBQwuyfc/Kpfk068oT8f4S0pQuEvWMvPH4w8ozudTI0p3ur2usbntB97VW+pYvbmWVZvrWL2llvdWVzF74dqdzgCORoyyIn9Wb//i4Izf4lz6t1tvve5TkKMzfiVhFO4iXcjPibL/gF7sP6BXp7c759hU08jHm2t9+G+pZW1VHeu3+hO9KrfVs2TtViq31dPY3Pmgc8X5sZ3G/Om9wzhAfr11SOvCXD+OUOsw1vk5Ef1ALJ1SuIt0k5lRWpRLaVHXh3iCfxOoqm2icls9lVvrffBvrWdzMARE+yEhPt5S64eFqGnco1FIzaAo148QWpQX88NXxyLkxiLktV2ibes7Xm9vj0UjxCJGNGJt19uXO7/Nr0f8dTRoM8MMIsEbTiRiGH49YkBwW2ubmd+P1mk7GP5+7beVPZOQcDezccCvgShwn3Pup4l4HJF0YGZ+hM7CnC6/BXTknKOmobntDaCmYfuw1a3X2zqsVzc0UxsMa93Q1MLWuiY2NPmhrBuaW6hvbNnhurmbQ1iHreMbgX/D8G8Erbdb27a2ferroL31DaKr7fzNnW2z82O0f7NpXexsu13V8rWTR3P2oYPj2ied6fFwN7MocDtwKrASeN3MZjnn3uvpxxLJVGbWNsPYkJKuz/6NR1Pz9rBvanE0tziaWlqCa7/e3NbuaG5poam53bpzNDe7ne7rHDigJVhoca5tvcUBwbVrvcYvuw7bOte+ffttXW1Lu9ug9e+2Lm9vb9uubdnfvn15ezvt2zv8rc7uzw7tHR+jQ3vQUFKQs/dP3h5IxCf3o4ClzrkPAMxsGjAeULiLpJBY1HfHFOaGXYkkQiIO2h0CfNRufWXQtgMzu9LMKsysorKyMgFliIhkr9DOyHDO3eOcK3fOlffv3z+sMkREMlIiwn0VMKzd+tCgTUREkiQR4f46MNrMRplZLnABMCsBjyMiIl3o8R9UnXNNZnYt8DT+UMgHnHPv9vTjiIhI1xJynLtz7kngyUT8bRER2T0NcScikoEU7iIiGchc+9O2wirCrBJY3s279wPW92A5PSlVa1Nde0d17b1UrS3T6hrhnOv0WPKUCPd4mFmFc6487Do6k6q1qa69o7r2XqrWlk11qVtGRCQDKdxFRDJQJoT7PWEXsAupWpvq2juqa++lam1ZU1fa97mLiMjOMuGTu4iIdKBwFxHJQGkd7mY2zswWm9lSM7spxDqGmdlzZvaemb1rZtcH7d83s1Vm9lZwOTOE2paZ2fzg8SuCtlIze8bMlgTXfZNc04Ht9slbZlZlZjeEtb/M7AEzW2dmC9q1dbqPzPtN8Jp7x8yOSHJdPzezRcFjP25mJUH7SDOrbbfv7kpyXV0+d2b27WB/LTaz0xNV1y5qe6RdXcvM7K2gPSn7bBf5kNjXmJ+WKv0u+EHJ/g3sC+QCbwMHhVTLIOCIYLkYeB84CPg+8M2Q99MyoF+Htp8BNwXLNwG3hPw8rtEYhPUAAANkSURBVAFGhLW/gBOAI4AFu9tHwJnA3/FTYI4F5ia5rtOAWLB8S7u6RrbfLoT91elzF/w/eBvIA0YF/2ejyaytw+2/BL6XzH22i3xI6GssnT+5t03n55xrAFqn80s659xq59y8YHkrsJBOZp9KIeOBKcHyFGBCiLWcDPzbOdfdM5Tj5px7AdjYobmrfTQeeNB5rwElZjYoWXU55/7hnGsKVl/Dz5eQVF3sr66MB6Y55+qdcx8CS/H/d5Nem5kZcD7wcKIev4uausqHhL7G0jnc92g6v2Qzs5HA4cDcoOna4KvVA8nu/gg44B9m9oaZXRm0DXTOrQ6W1wADQ6ir1QXs+J8t7P3Vqqt9lEqvu0vxn/BajTKzN83sn2Z2fAj1dPbcpdL+Oh5Y65xb0q4tqfusQz4k9DWWzuGecsysFzADuME5VwXcCewHHAasxn8lTLbjnHNHAGcA15jZCe1vdP57YCjHw5qfzOVs4M9BUyrsr52EuY+6YmbfAZqAqUHTamC4c+5w4EbgT2bWO4klpeRz18EkdvwgkdR91kk+tEnEayydwz2lpvMzsxz8EzfVOfcYgHNurXOu2TnXAtxLAr+OdsU5tyq4Xgc8HtSwtvVrXnC9Ltl1Bc4A5jnn1gY1hr6/2ulqH4X+ujOzLwFnARcGoUDQ7bEhWH4D37d9QLJq2sVzF/r+AjCzGHAO8EhrWzL3WWf5QIJfY+kc7ikznV/Ql3c/sNA5d2u79vb9ZJ8HFnS8b4LrKjKz4tZl/I9xC/D7aXKw2WRgZjLrameHT1Jh768OutpHs4BLgiMaxgJb2n21TjgzGwd8CzjbOVfTrr2/mUWD5X2B0cAHSayrq+duFnCBmeWZ2aigrn8lq652TgEWOedWtjYka591lQ8k+jWW6F+KE3nB/6r8Pv4d9zsh1nEc/ivVO8BbweVM4CFgftA+CxiU5Lr2xR+p8Dbwbus+AsqAOcASYDZQGsI+KwI2AH3atYWyv/BvMKuBRnz/5mVd7SP8EQy3B6+5+UB5kutaiu+PbX2d3RVsOzF4jt8C5gGfS3JdXT53wHeC/bUYOCPZz2XQ/gfgqg7bJmWf7SIfEvoa0/ADIiIZKJ27ZUREpAsKdxGRDKRwFxHJQAp3EZEMpHAXEclACncRkQykcBcRyUD/H/doUYpS4JSwAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Insert your code for getting overfitting, underfitting and just right \n",
        "\n",
        "#Overfitting: \n",
        "\n",
        "x_train_done = x_train[:10]\n",
        "y_train_done = y_train[:10]\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train_done, forward(x_train_done, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "    \n",
        "    if e%10==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "print(\"Another option would be infinitly high EPOCHS, but that takes time\")\n",
        "\n",
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Underfitting: \n",
        "print(\"Underfitting\")\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "EPOCHS = 30\n",
        "LEARN_R = 2e-4\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "    \n",
        "    if e%10==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qxxcqnPG6piy",
        "outputId": "f6c7f2ab-79a0-47da-87f9-245bb473abd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Underfitting\n",
            "   0 ( 0.00%) Train loss: 109.936 \t Validation loss: 154.266\n",
            "  10 (33.33%) Train loss: 106.407 \t Validation loss: 150.251\n",
            "  20 (66.67%) Train loss: 104.749 \t Validation loss: 148.131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Underfitting: \n",
        "print(\"Good fitting\")\n",
        "NN = [\n",
        "    DenseLayer(1, 8, lambda x: x.relu()),\n",
        "    DenseLayer(8, 1, lambda x: x.identity())\n",
        "]\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "EPOCHS = 200\n",
        "LEARN_R = 2e-3\n",
        "for e in range(EPOCHS):\n",
        "     \n",
        "    # Forward pass and loss computation\n",
        "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
        "\n",
        "    # Backward pass\n",
        "    Loss.backward()\n",
        "    \n",
        "    # gradient descent update\n",
        "    update_parameters(parameters(NN), LEARN_R)\n",
        "    zero_gradients(parameters(NN))\n",
        "    \n",
        "    # Training loss\n",
        "    train_loss.append(Loss.v)\n",
        "    \n",
        "    # Validation\n",
        "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
        "    val_loss.append(Loss_validation.v)\n",
        "    \n",
        "    if e%10==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BqXs1q5x8s7a",
        "outputId": "b7ccd84a-9728-4ca8-beb7-9396081fc7d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good fitting\n",
            "   0 ( 0.00%) Train loss: 104.759 \t Validation loss: 141.945\n",
            "  10 ( 5.00%) Train loss: 30.250 \t Validation loss: 32.140\n",
            "  20 (10.00%) Train loss: 13.677 \t Validation loss: 16.324\n",
            "  30 (15.00%) Train loss: 12.816 \t Validation loss: 15.340\n",
            "  40 (20.00%) Train loss: 12.427 \t Validation loss: 14.980\n",
            "  50 (25.00%) Train loss: 12.168 \t Validation loss: 14.694\n",
            "  60 (30.00%) Train loss: 11.996 \t Validation loss: 14.457\n",
            "  70 (35.00%) Train loss: 11.780 \t Validation loss: 14.161\n",
            "  80 (40.00%) Train loss: 11.597 \t Validation loss: 13.895\n",
            "  90 (45.00%) Train loss: 11.456 \t Validation loss: 13.677\n",
            " 100 (50.00%) Train loss: 11.328 \t Validation loss: 13.497\n",
            " 110 (55.00%) Train loss: 11.215 \t Validation loss: 13.337\n",
            " 120 (60.00%) Train loss: 11.154 \t Validation loss: 13.229\n",
            " 130 (65.00%) Train loss: 11.114 \t Validation loss: 13.154\n",
            " 140 (70.00%) Train loss: 11.095 \t Validation loss: 13.111\n",
            " 150 (75.00%) Train loss: 11.085 \t Validation loss: 13.087\n",
            " 160 (80.00%) Train loss: 11.078 \t Validation loss: 13.065\n",
            " 170 (85.00%) Train loss: 11.074 \t Validation loss: 13.047\n",
            " 180 (90.00%) Train loss: 11.071 \t Validation loss: 13.032\n",
            " 190 (95.00%) Train loss: 11.065 \t Validation loss: 13.009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPZP-eTFtIo"
      },
      "source": [
        "# Next steps - classification\n",
        "\n",
        "It is straight forward to extend what we have done to classification. \n",
        "\n",
        "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
        "\n",
        "Next week we will see how to perform classification in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVPul3QFtIo"
      },
      "source": [
        "## Exercise l) optional - Implement backpropagation for classification\n",
        "\n",
        "Should be possible with very few lines of code. :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "oC8QrI2tFtIp"
      },
      "outputs": [],
      "source": [
        "# Just add code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APqhJv3tta1O"
      },
      "source": [
        "## Exercise m) optional - Introduce a NeuralNetwork class\n",
        "\n",
        "The functions we applied on the neural network (parameters, update_parameters and zero_gradients) can more naturally be included as methods in a NeuralNetwork class. Make such a class and modify the code to use it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "Dqfnor1ouMLq"
      },
      "outputs": [],
      "source": [
        "# just add some code"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "U4057_ljNvWB",
        "p_8n_SKnIW2F",
        "oLrGJytZFtGm",
        "jpIZPBpNI0pO",
        "_79HOAXrFtHK",
        "mqeyab9qFtGs",
        "-XyXBD37FtHk",
        "SrwSJ2UWFtHu",
        "zTBAmjsAFtIk",
        "qsVPul3QFtIo",
        "APqhJv3tta1O"
      ],
      "name": "2.1-EXE-FNN-AutoDif-Nanograd.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}